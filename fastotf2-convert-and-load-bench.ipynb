{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "18ba373d",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e8e4a94b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Roundtrip module could not be loaded. Requires jupyter notebook version <= 7.x.\n"
     ]
    }
   ],
   "source": [
    "from typing import Dict, List, Tuple, Optional\n",
    "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
    "\n",
    "from scipy.signal import savgol_filter, medfilt\n",
    "\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "from tqdm import tqdm\n",
    "import thicket as tt\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5b62064c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Notebook started at 1767993847.1379 (2026-01-09 16:24:07.137900)\n"
     ]
    }
   ],
   "source": [
    "# Track how long the notebook takes to run\n",
    "NOTEBOOK_START_TIME = time.time()\n",
    "print(f\"Notebook started at {NOTEBOOK_START_TIME} ({datetime.datetime.fromtimestamp(NOTEBOOK_START_TIME)})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8a9f753f",
   "metadata": {},
   "outputs": [],
   "source": [
    "METRICS_TO_TRACK=\"A2rocm_smi:::energy_count:device=0,A2rocm_smi:::energy_count:device=2,A2rocm_smi:::energy_count:device=4,A2rocm_smi:::energy_count:device=6,A2rocm_smi:::gpu_clk_freq_System:device=0:current,A2rocm_smi:::gpu_clk_freq_System:device=2:current,A2rocm_smi:::gpu_clk_freq_System:device=4:current,A2rocm_smi:::gpu_clk_freq_System:device=6:current,A2rocm_smi:::temp_current:device=0:sensor=1,A2rocm_smi:::temp_current:device=2:sensor=1,A2rocm_smi:::temp_current:device=4:sensor=1,A2rocm_smi:::temp_current:device=6:sensor=1,A2rocm_smi:::power_average:device=0:sensor=0,A2rocm_smi:::power_average:device=2:sensor=0,A2rocm_smi:::power_average:device=4:sensor=0,A2rocm_smi:::power_average:device=6:sensor=0,A2rocm_smi:::memory_busy_percent:device=0,A2rocm_smi:::memory_busy_percent:device=2,A2rocm_smi:::memory_busy_percent:device=4,A2rocm_smi:::memory_busy_percent:device=6,A2rocm_smi:::busy_percent:device=0,A2rocm_smi:::busy_percent:device=2,A2rocm_smi:::busy_percent:device=4,A2rocm_smi:::busy_percent:device=6,A2coretemp:::craypm:power,A2coretemp:::craypm:energy,A2coretemp:::craypm:freshness,A2coretemp:::craypm:cpu_energy,A2coretemp:::craypm:cpu_power,A2coretemp:::craypm:memory_energy,A2coretemp:::craypm:memory_power,A2coretemp:::craypm:accel0_energy,A2coretemp:::craypm:accel0_energy_timestamp,A2coretemp:::craypm:accel0_power,A2coretemp:::craypm:accel1_energy,A2coretemp:::craypm:accel1_energy_timestamp,A2coretemp:::craypm:accel1_power,A2coretemp:::craypm:accel2_energy,A2coretemp:::craypm:accel2_energy_timestamp,A2coretemp:::craypm:accel2_power,A2coretemp:::craypm:accel3_energy,A2coretemp:::craypm:accel3_energy_timestamp,A2coretemp:::craypm:accel3_power\".split(\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "36c97ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "def metrics_to_csv(group: str, thread_metrics: Dict[str, List[Tuple[float, float]]], filename: str):\n",
    "    \"\"\"Convert metrics to a CSV file.\"\"\"\n",
    "    with open(filename, 'w') as f:\n",
    "        f.write(\"Group,Metric Name,Time,Value\\n\")\n",
    "        for metric_name, values in thread_metrics.items():\n",
    "            for time, value in values:\n",
    "                f.write(f\"{group},{metric_name},{time},{value}\\n\")\n",
    "\n",
    "def read_metrics_dataframe(file: str) -> pd.DataFrame:\n",
    "    dtype_map = {\n",
    "        'Group': 'category',\n",
    "        'Metric Name': 'category',\n",
    "        'Time': 'float64',\n",
    "        'Value': 'int64' # Int64 for precision\n",
    "    }\n",
    "    df = pd.read_csv(file, dtype=dtype_map)\n",
    "    df = df[df['Metric Name'].isin(METRICS_TO_TRACK)]\n",
    "    # For all the rocm metrics, divide by 1e6 to convert from uJ to J\n",
    "    for metric_name in df['Metric Name'].unique():\n",
    "        if 'rocm_smi:::energy_count:device' in metric_name or 'rocm_smi:::power_average:device' in metric_name or 'rocm_smi:::current_socket_power:device' in metric_name:\n",
    "            df['Value'] = df['Value'].astype(float)\n",
    "            df['Value'] = df['Value'].fillna(0.0)\n",
    "            df.loc[df['Metric Name'] == metric_name, 'Value'] /= 1_000_000\n",
    "    return df\n",
    "\n",
    "def read_call_graph_dataframe(file: str) -> pd.DataFrame:\n",
    "    dtype_map = {\n",
    "        'Thread': 'category',\n",
    "        'Group': 'category',\n",
    "        'Depth': 'uint32',\n",
    "        'Name': 'category',\n",
    "        'Start Time': 'float64',\n",
    "        'End Time': 'float64',\n",
    "        'Duration': 'float64'\n",
    "    }\n",
    "    return pd.read_csv(file, dtype=dtype_map)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5805e3ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sampled_to_continuous(df, time_col='Time', value_col='Value'):\n",
    "    t_samples = df[time_col].astype(float)\n",
    "    y = df[value_col].astype(float)\n",
    "    # Check if y is empty\n",
    "    if y.empty:\n",
    "        print(\"Warning: Empty data for sampled_to_continuous. Returning zero function.\")\n",
    "        return lambda t: 0.0\n",
    "    f = lambda t: np.interp(t, t_samples, y)\n",
    "    return f\n",
    "\n",
    "def numerical_derivative(func, h=1e-5):\n",
    "    return lambda t: (func(t + h) - func(t - h)) / (2 * h)\n",
    "\n",
    "def numerical_integral(f, a=0.0, dx=1e-1):\n",
    "    # Determinar si f soporta arrays de NumPy\n",
    "    try:\n",
    "        _ = f(np.array([a, a + dx]))\n",
    "        f_np = f\n",
    "    except Exception:\n",
    "        f_np = np.vectorize(f)\n",
    "    \n",
    "    def F(x):\n",
    "        x_arr = np.atleast_1d(x).astype(float)\n",
    "        sign = np.ones_like(x_arr)\n",
    "        mask = x_arr < a\n",
    "        sign[mask] = -1\n",
    "        x_pos = np.where(mask, a + (a - x_arr), x_arr)\n",
    "        \n",
    "        xmax = x_pos.max()\n",
    "        t = np.arange(a, xmax + dx, dx)\n",
    "        u = f_np(t)\n",
    "        \n",
    "        trap_heights = (u[:-1] + u[1:]) / 2\n",
    "        cum = np.concatenate(([0], np.cumsum(trap_heights * dx)))\n",
    "        \n",
    "        idx = ((x_pos - a) // dx).astype(int)\n",
    "        rem = x_pos - (a + idx * dx)\n",
    "        \n",
    "        f_end = f_np(x_pos)\n",
    "        f_start = u[idx]\n",
    "        \n",
    "        integral = cum[idx] + (f_start + f_end) * rem / 2\n",
    "        result = sign * integral\n",
    "        \n",
    "        return result.item() if np.isscalar(x) else result\n",
    "    \n",
    "    return F\n",
    "        \n",
    "def continuous_savgol_filter(f, window_length=11, polyorder=2, sample_rate=0.001):\n",
    "    def f_smooth(t):\n",
    "        t = np.asarray(t)\n",
    "        t_min = t.min()\n",
    "        t_max = t.max()\n",
    "        t_samples = np.arange(t_min, t_max + sample_rate, sample_rate)\n",
    "        y_samples = f(t_samples)\n",
    "        y_smooth = savgol_filter(y_samples, window_length=window_length, polyorder=polyorder, mode='nearest')\n",
    "        return np.interp(t, t_samples, y_smooth)\n",
    "    return f_smooth\n",
    "\n",
    "def continuous_savgol_derivative(f, window_length=15, polyorder=3, sample_rate=0.001):\n",
    "    def f_prime(t):\n",
    "        t = np.asarray(t)\n",
    "        \n",
    "        t_min = t.min()\n",
    "        t_max = t.max()\n",
    "        \n",
    "        if (t_max - t_min) < (window_length * sample_rate):\n",
    "            pad = (window_length * sample_rate) / 2\n",
    "            t_min -= pad\n",
    "            t_max += pad\n",
    "\n",
    "        t_samples = np.arange(t_min, t_max + sample_rate, sample_rate)\n",
    "        \n",
    "        y_samples = f(t_samples)\n",
    "        \n",
    "        y_deriv = savgol_filter(\n",
    "            y_samples, \n",
    "            window_length=window_length, \n",
    "            polyorder=polyorder, \n",
    "            deriv=1, \n",
    "            delta=sample_rate, \n",
    "            mode='nearest'\n",
    "        )\n",
    "        \n",
    "        return np.interp(t, t_samples, y_deriv)\n",
    "        \n",
    "    return f_prime\n",
    "        \n",
    "def continous_median_filter(f, kernel_size=11, sample_rate=0.001):\n",
    "    def f_smooth(t):\n",
    "        t = np.asarray(t)\n",
    "        t_min = t.min()\n",
    "        t_max = t.max()\n",
    "        t_samples = np.arange(t_min, t_max + sample_rate, sample_rate)\n",
    "        y_samples = f(t_samples)\n",
    "        y_smooth = medfilt(y_samples, kernel_size=kernel_size)\n",
    "        return np.interp(t, t_samples, y_smooth)\n",
    "    return f_smooth        \n",
    "\n",
    "def unwrap_or_zero(f):\n",
    "    if not f:\n",
    "        return 0.0\n",
    "    else:\n",
    "        return f\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79d8e8d5",
   "metadata": {},
   "source": [
    "### Thicket + Hatchet Integration\n",
    "Convert the CSV datasets into thicket/hatchet objects for analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e53eb5bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_hatchet_dag(call_graph: pd.DataFrame, metric_functions: Dict[str, callable]):\n",
    "    df = call_graph.copy()\n",
    "    # Asume que viene “bien formado”: cada fila es una invocación con Depth que respeta el anidamiento.\n",
    "    # Si no está ordenado por Start Time, ordénalo una vez.\n",
    "    df = df.sort_values(by='Start Time')\n",
    "\n",
    "    # Filtrado opcional por duración, replica tu lógica si quieres:\n",
    "    df = df[df['Duration'] >= 0.001]\n",
    "\n",
    "    pref3 = df['Name'].str[:3].str.lower()\n",
    "    df = df[~pref3.isin(['hip', 'mpi'])]\n",
    "    \n",
    "    # Vectoriza cálculo de métricas por fila (Δ = f(end) - f(start))\n",
    "    starts = df['Start Time'].to_numpy()\n",
    "    ends   = df['End Time'].to_numpy()\n",
    "    names  = df['Name'].to_numpy()\n",
    "    depths = df['Depth'].to_numpy()\n",
    "    durs   = df['Duration'].to_numpy()\n",
    "\n",
    "\n",
    "    metrics_cols = {}\n",
    "    for metric, f in tqdm(metric_functions.items()):\n",
    "        try:\n",
    "            vals = f(ends) - f(starts)            # ideal: f soporta arrays\n",
    "            vals = np.asarray(vals, dtype=float)\n",
    "            vals = np.nan_to_num(vals, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "        except Exception:\n",
    "            # Fallback si f no vectoriza\n",
    "            vals = np.fromiter(\n",
    "                (unwrap_or_zero(f(e) - f(s)) for s, e in zip(starts, ends)),\n",
    "                dtype=float, count=len(starts)\n",
    "            )\n",
    "        metrics_cols[metric] = vals\n",
    "\n",
    "    # Construcción del DAG con una pila indexada por profundidad\n",
    "    # Mantenemos las referencias a los últimos nodos por cada Depth.\n",
    "    # Cuando Depth sube, el nuevo nodo se anexa como hijo del nodo en Depth-1.\n",
    "    dag_roots: list = []\n",
    "    last_at_depth: dict[int, dict] = {}\n",
    "\n",
    "    # Usamos itertuples para bajo overhead\n",
    "    M = len(df)\n",
    "    for i, (name, depth, dur) in enumerate(zip(names, depths, durs)):\n",
    "        node_metrics = {'time (inc)': float(dur), 'time': 0.0}\n",
    "        for m, arr in metrics_cols.items():\n",
    "            node_metrics[m] = float(arr[i])\n",
    "\n",
    "        node = {'frame': {'name': name}, 'metrics': node_metrics, 'children': []}\n",
    "\n",
    "        if depth == 0 or (depth - 1) not in last_at_depth:\n",
    "            dag_roots.append(node)\n",
    "        else:\n",
    "            parent = last_at_depth[depth - 1]\n",
    "            parent['children'].append(node)\n",
    "\n",
    "        last_at_depth[depth] = node\n",
    "\n",
    "        # Si retrocede la profundidad en la siguiente fila, iremos sobreescribiendo last_at_depth\n",
    "        # No hace falta “cerrar” nada aún; calculamos 'time' más adelante en una segunda pasada.\n",
    "\n",
    "    # Segunda pasada postorden para calcular 'time' = 'time (inc)' - suma hijos.\n",
    "    # Podemos hacerlo con una DFS iterativa.\n",
    "    stack = [(root, False) for root in dag_roots]\n",
    "    while stack:\n",
    "        node, visited = stack.pop()\n",
    "        if not visited:\n",
    "            stack.append((node, True))\n",
    "            for ch in node['children']:\n",
    "                stack.append((ch, False))\n",
    "        else:\n",
    "            child_incs = 0.0\n",
    "            for ch in node['children']:\n",
    "                child_incs += ch['metrics']['time (inc)']\n",
    "            node['metrics']['time'] = node['metrics']['time (inc)'] - child_incs\n",
    "\n",
    "    return dag_roots"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af718788",
   "metadata": {},
   "source": [
    "# Hierarchical DataFrames\n",
    "\n",
    "Organize the data into hierarchical dataframes for easier analysis:\n",
    "- `Ensemble`: all the runs of a given configuration\n",
    "- `Run`: a single run of a configuration\n",
    "- `Node`: a single node in a run\n",
    "- `Rank`: a single MPI rank in a node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "50f43985",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Rank:\n",
    "    def __init__(self, node: str, name: str, call_graph: pd.DataFrame):\n",
    "        self.node = node\n",
    "        self.name = name\n",
    "        self.call_graph = call_graph\n",
    "        assert self.call_graph.columns.tolist() == ['Thread', 'Group', 'Depth', 'Name', 'Start Time', 'End Time', 'Duration'], f\"Call graph DataFrame has incorrect columns, found {call_graph.columns.tolist()}\"\n",
    "        \n",
    "    def get_all_code_regions(self) -> List[str]:\n",
    "        return self.call_graph['Name'].unique().tolist()\n",
    "\n",
    "    def compute_attribution(\n",
    "        self,\n",
    "        metrics_accumulated: Dict[str, callable],\n",
    "        duration_threshold: float = 0.001,\n",
    "        weights_by_metric: Optional[Dict[str, np.ndarray]] = None):\n",
    "        df = self.call_graph\n",
    "        df = df[df['Duration'] >= duration_threshold]\n",
    "        pref3 = df['Name'].str[:3].str.lower()\n",
    "        df = df[~pref3.isin(['hip', 'mpi'])]\n",
    "\n",
    "        if df.empty:\n",
    "            return pd.DataFrame(columns=['Code Region'] + list(metrics_accumulated.keys()))\n",
    "\n",
    "        starts = df['Start Time'].to_numpy()\n",
    "        ends   = df['End Time'].to_numpy()\n",
    "        names  = df['Name'].to_numpy()\n",
    "\n",
    "        cols = {}\n",
    "        for metric, f in metrics_accumulated.items():\n",
    "            try:\n",
    "                deltas = f(ends) - f(starts)\n",
    "                deltas = np.asarray(deltas, dtype=float)\n",
    "                deltas = np.nan_to_num(deltas, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "            except Exception:\n",
    "                deltas = np.fromiter(\n",
    "                    (unwrap_or_zero(f(e) - f(s)) for s, e in zip(starts, ends)),\n",
    "                    dtype=float, count=len(starts)\n",
    "                )\n",
    "\n",
    "            if weights_by_metric and metric in weights_by_metric:\n",
    "                w = np.asarray(weights_by_metric[metric], dtype=float)\n",
    "                if w.shape[0] == deltas.shape[0]:\n",
    "                    deltas = deltas * w\n",
    "                elif w.size == 1:\n",
    "                    deltas = deltas * float(w)\n",
    "                else:\n",
    "                    deltas = deltas * 1.0\n",
    "\n",
    "            cols[metric] = deltas\n",
    "\n",
    "        out = pd.DataFrame(cols)\n",
    "        out['Code Region'] = names\n",
    "        out = out.groupby('Code Region', as_index=False).sum()\n",
    "        return out\n",
    "\n",
    "    def refresh(self):\n",
    "        return Rank(self.node, self.name, self.call_graph)\n",
    "\n",
    "    def __str__(self):\n",
    "        return f\"Rank({self.name})\"\n",
    "\n",
    "def build_inclusive_masks_for_rank(breaks, s, e, names, depths):\n",
    "    \"\"\"\n",
    "    For each segment [breaks[i], breaks[i+1]), return a dict:\n",
    "      masks[region_name][i] = True  iff the region is on the call stack\n",
    "                                   (ancestor of top-of-stack) for the ENTIRE segment.\n",
    "    Assumptions:\n",
    "      - breaks includes all start/end times (so inside a segment no boundary occurs).\n",
    "      - 'depths' is 0-based, increasing with nesting.\n",
    "    \"\"\"\n",
    "    seg_count = breaks.size - 1\n",
    "    masks = defaultdict(lambda: np.zeros(seg_count, dtype=bool))\n",
    "\n",
    "    lefts, rights = breaks[:-1], breaks[1:]\n",
    "    for i in range(seg_count):\n",
    "        L, R = lefts[i], rights[i]\n",
    "\n",
    "        # Regions that cover the whole segment (active across the entire segment)\n",
    "        idx = np.where((s <= L) & (e >= R))[0]\n",
    "        if idx.size == 0:\n",
    "            continue\n",
    "\n",
    "        # Top of stack depth\n",
    "        dmax = depths[idx].max()\n",
    "\n",
    "        # Inclusive chain = exactly one region per depth 0..dmax that covers the whole segment\n",
    "        # (there should typically be at most one per depth due to proper nesting)\n",
    "        for d in range(dmax + 1):\n",
    "            cand = idx[depths[idx] == d]\n",
    "            if cand.size:\n",
    "                # If instrumentation duplicates exist at same depth, pick the one that covers the segment;\n",
    "                # here cand already covers segment; pick the first deterministically.\n",
    "                j = cand[0]\n",
    "                masks[names[j]][i] = True\n",
    "\n",
    "    return masks\n",
    "\n",
    "class Node:\n",
    "    def __init__(self, name: str, metrics_df: pd.DataFrame, ranks: List[Rank]):\n",
    "        self.name = name\n",
    "        self.ranks = ranks\n",
    "        self.metrics = metrics_df\n",
    "        assert self.metrics.columns.tolist() == ['Group', 'Metric Name', 'Time', 'Value'], f\"Metrics DataFrame has incorrect columns, found {metrics_df.columns.tolist()}\"\n",
    "\n",
    "    def refresh(self):\n",
    "        return Node(self.name, self.metrics, [rank.refresh() for rank in self.ranks])\n",
    "\n",
    "    def get_metric_names(self) -> List[str]:\n",
    "        return self.metrics['Metric Name'].unique().tolist()\n",
    "    \n",
    "    def get_metric_samples(self, metric_name: str) -> pd.DataFrame:\n",
    "        return self.metrics[self.metrics['Metric Name'] == metric_name]\n",
    "\n",
    "    def _filtered_callgraph(self, rank: Rank, duration_threshold: float):\n",
    "        df = rank.call_graph\n",
    "        df = df[df['Duration'] >= duration_threshold]\n",
    "        pref3 = df['Name'].str[:3].str.lower()\n",
    "        # df = df[~pref3.isin(['hip', 'mpi'])]\n",
    "        # Finite and non-degenerate\n",
    "        mfin = np.isfinite(df['Start Time']) & np.isfinite(df['End Time'])\n",
    "        df = df[mfin & (df['End Time'] > df['Start Time'])]\n",
    "        return df\n",
    "\n",
    "    # ---- NEW CORE HELPER: inclusive vs exclusive controlled by flag ----\n",
    "    def _compute_attribution_core(\n",
    "        self,\n",
    "        ranks_to_metrics: Dict[str, List[str]],\n",
    "        instantaneous_metrics = [],\n",
    "        duration_threshold: float = 0.0,\n",
    "        inclusive: bool = True,\n",
    "    ):\n",
    "        used_metrics = sorted({m for ml in ranks_to_metrics.values() for m in ml})\n",
    "        if not used_metrics:\n",
    "            return pd.DataFrame(columns=['Code Region'])\n",
    "\n",
    "        metrics_continuous = {m: sampled_to_continuous(self.get_metric_samples(m)) for m in used_metrics}\n",
    "        metrics_accumulated = {\n",
    "            m: (numerical_integral(metrics_continuous[m]) if m in instantaneous_metrics else metrics_continuous[m])\n",
    "            for m in used_metrics\n",
    "        }\n",
    "\n",
    "        rank_df = {r.name: self._filtered_callgraph(r, duration_threshold) for r in self.ranks}\n",
    "        rank_events = {\n",
    "            rname: (\n",
    "                df['Start Time'].to_numpy(),\n",
    "                df['End Time'].to_numpy(),\n",
    "                df['Name'].to_numpy(),\n",
    "                df['Depth'].astype(int).to_numpy()\n",
    "            )\n",
    "            for rname, df in rank_df.items()\n",
    "        }\n",
    "\n",
    "        metric_to_ranks: Dict[str, List[str]] = {}\n",
    "        for rname, mlist in ranks_to_metrics.items():\n",
    "            for m in mlist:\n",
    "                metric_to_ranks.setdefault(m, []).append(rname)\n",
    "\n",
    "        rank_tables: Dict[str, Dict[str, Dict[str, float]]] = {r.name: {} for r in self.ranks}\n",
    "\n",
    "        for metric in used_metrics:\n",
    "            f = metrics_accumulated[metric]\n",
    "            rnames = [rn for rn in metric_to_ranks.get(metric, []) if rn in rank_events]\n",
    "            if not rnames:\n",
    "                continue\n",
    "\n",
    "            all_s, all_e = [], []\n",
    "            for rn in rnames:\n",
    "                s, e, _, _ = rank_events[rn]\n",
    "                if s.size:\n",
    "                    all_s.append(s); all_e.append(e)\n",
    "            if not all_s:\n",
    "                continue\n",
    "\n",
    "            S_concat = np.concatenate(all_s)\n",
    "            E_concat = np.concatenate(all_e)\n",
    "            breaks = np.unique(np.concatenate([S_concat, E_concat]))\n",
    "            breaks = breaks[np.isfinite(breaks)]\n",
    "            if breaks.size < 2:\n",
    "                continue\n",
    "            seg_count = breaks.size - 1\n",
    "\n",
    "            # dE per segment\n",
    "            try:\n",
    "                E_vals = f(breaks)\n",
    "                E_vals = np.asarray(E_vals, dtype=float)\n",
    "                if E_vals.shape != breaks.shape:\n",
    "                    raise ValueError\n",
    "            except Exception:\n",
    "                print(f\"Warning: Non-vectorized metric function for metric {metric}, falling back to loop.\")\n",
    "                E_vals = np.array([float(f(float(t))) for t in breaks], dtype=float)\n",
    "\n",
    "            dE = E_vals[1:] - E_vals[:-1]\n",
    "            dE = np.nan_to_num(dE, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "            dE = np.maximum(dE, 0.0)\n",
    "\n",
    "            # Which ranks are active in each segment?\n",
    "            active_by_rank: Dict[str, np.ndarray] = {}\n",
    "            lr_by_rank: Dict[str, Tuple[np.ndarray, np.ndarray]] = {}\n",
    "\n",
    "            zeros_seg = np.zeros(seg_count, dtype=bool)\n",
    "            for rn in rnames:\n",
    "                s, e, _, _ = rank_events[rn]\n",
    "                if s.size == 0:\n",
    "                    active_by_rank[rn] = zeros_seg\n",
    "                    lr_by_rank[rn] = (np.empty(0, dtype=int), np.empty(0, dtype=int))\n",
    "                    continue\n",
    "\n",
    "                L = np.searchsorted(breaks, s, side='right') - 1\n",
    "                R = np.searchsorted(breaks, e, side='left') - 1\n",
    "                valid = (R >= L) & (R >= 0) & (L < seg_count)\n",
    "                L = np.clip(L[valid], 0, seg_count - 1)\n",
    "                R = np.clip(R[valid], 0, seg_count - 1)\n",
    "\n",
    "                lr_by_rank[rn] = (L, R)\n",
    "\n",
    "                # coverage count via difference array\n",
    "                diff = np.zeros(seg_count + 1, dtype=int)\n",
    "                for l, r in zip(L, R):\n",
    "                    diff[l] += 1\n",
    "                    diff[r + 1] -= 1\n",
    "                c = np.cumsum(diff[:-1])\n",
    "                active_by_rank[rn] = (c > 0)\n",
    "\n",
    "            # k(t): number of active ranks per segment\n",
    "            k = np.zeros(seg_count, dtype=int)\n",
    "            for rn in rnames:\n",
    "                k += active_by_rank[rn].astype(int)\n",
    "\n",
    "            # Energy share per segment for each rank: ΔE/k (if k==0, remains unattributed)\n",
    "            share_per_seg = np.zeros_like(dE, dtype=float)\n",
    "            mask_k = (k > 0)\n",
    "            share_per_seg[mask_k] = dE[mask_k] / k[mask_k]\n",
    "\n",
    "            # --- Attribution per rank ---\n",
    "            for rn in rnames:\n",
    "                s, e, names, depths = rank_events[rn]\n",
    "                if s.size == 0:\n",
    "                    continue\n",
    "\n",
    "                L, R = lr_by_rank[rn]\n",
    "                if L.size == 0:\n",
    "                    continue\n",
    "\n",
    "                if inclusive:\n",
    "                    masks: Dict[str, np.ndarray] = defaultdict(lambda: np.zeros(seg_count, dtype=bool))\n",
    "                    for j in range(L.size):\n",
    "                        l = L[j]; r = R[j]\n",
    "                        if r < l:\n",
    "                            continue\n",
    "                        masks[names[j]][l:r+1] = True\n",
    "\n",
    "                    for region_name, seg_mask in masks.items():\n",
    "                        if not seg_mask.any():\n",
    "                            continue\n",
    "                        contrib = float(share_per_seg[seg_mask].sum())\n",
    "                        if contrib == 0.0:\n",
    "                            continue\n",
    "                        tbl = rank_tables[rn].setdefault(region_name, {})\n",
    "                        tbl[metric] = tbl.get(metric, 0.0) + contrib\n",
    "                else:\n",
    "                    top_depth = np.full(seg_count, -1, dtype=int)\n",
    "                    top_idx   = np.full(seg_count, -1, dtype=int)\n",
    "\n",
    "                    # For each event j, mark segments [L[j], R[j]] and keep the deepest\n",
    "                    for j in range(L.size):\n",
    "                        l = L[j]; r = R[j]\n",
    "                        if r < l:\n",
    "                            continue\n",
    "                        d = depths[j]\n",
    "                        seg_slice = slice(l, r + 1)\n",
    "\n",
    "                        current_depths = top_depth[seg_slice]\n",
    "                        mask = d >= current_depths\n",
    "                        if not np.any(mask):\n",
    "                            continue\n",
    "\n",
    "                        # Update depths\n",
    "                        current_depths[mask] = d\n",
    "                        top_depth[seg_slice] = current_depths\n",
    "\n",
    "                        # Update indices\n",
    "                        current_idx = top_idx[seg_slice]\n",
    "                        current_idx[mask] = j\n",
    "                        top_idx[seg_slice] = current_idx\n",
    "\n",
    "                    # Now attribute share_per_seg only to top-of-stack region per segment\n",
    "                    for seg_i, j in enumerate(top_idx):\n",
    "                        if j < 0:\n",
    "                            continue  # rank not active here\n",
    "                        contrib = float(share_per_seg[seg_i])\n",
    "                        if contrib == 0.0:\n",
    "                            continue\n",
    "                        region_name = names[j]\n",
    "                        tbl = rank_tables[rn].setdefault(region_name, {})\n",
    "                        tbl[metric] = tbl.get(metric, 0.0) + contrib\n",
    "\n",
    "        # --- Convert to per-rank DataFrames and do a wide merge ---\n",
    "        attribution = None\n",
    "        for rank in self.ranks:\n",
    "            rn = rank.name\n",
    "            # Filter only the metrics requested for this rank\n",
    "            wanted = ranks_to_metrics.get(rn, [])\n",
    "            if not wanted:\n",
    "                continue\n",
    "\n",
    "            rows = []\n",
    "            for region, mdict in rank_tables.get(rn, {}).items():\n",
    "                row = {'Code Region': region}\n",
    "                # Ensure a column for each requested metric\n",
    "                for m in wanted:\n",
    "                    row[m] = mdict.get(m, 0.0)\n",
    "                rows.append(row)\n",
    "\n",
    "            df_rank = pd.DataFrame(rows) if rows else pd.DataFrame({'Code Region': []})\n",
    "            # Ensure columns if they are missing\n",
    "            for m in wanted:\n",
    "                if m not in df_rank.columns:\n",
    "                    df_rank[m] = 0.0\n",
    "\n",
    "            # Rename metrics with rank suffix to avoid collisions\n",
    "            rename_map = {m: f\"{m}_{rn}\" for m in wanted}\n",
    "            df_rank = df_rank.rename(columns=rename_map)\n",
    "\n",
    "            # Outer merge on 'Code Region'\n",
    "            attribution = df_rank if attribution is None else pd.merge(\n",
    "                attribution, df_rank, on='Code Region', how='outer'\n",
    "            )\n",
    "\n",
    "        # If there was nothing, return minimal header\n",
    "        if attribution is None:\n",
    "            all_cols = ['Code Region'] + [f\"{m}_{r.name}\" for r, ml in ranks_to_metrics.items() for m in ml]\n",
    "            return pd.DataFrame(columns=all_cols)\n",
    "\n",
    "        return attribution.fillna(0.0)\n",
    "\n",
    "    def compute_attribution(\n",
    "        self,\n",
    "        ranks_to_metrics: Dict[str, List[str]] = {},\n",
    "        instantaneous_metrics = [],\n",
    "        duration_threshold: float = 0.001,\n",
    "    ):\n",
    "        return self._compute_attribution_core(\n",
    "            ranks_to_metrics=ranks_to_metrics,\n",
    "            instantaneous_metrics=instantaneous_metrics,\n",
    "            duration_threshold=duration_threshold,\n",
    "            inclusive=True,\n",
    "        )\n",
    "\n",
    "    def compute_attribution_exclusive(\n",
    "        self,\n",
    "        ranks_to_metrics: Dict[str, List[str]] = {},\n",
    "        instantaneous_metrics = [],\n",
    "        duration_threshold: float = 0.001,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Exclusive attribution: for each segment and rank, only the *deepest*\n",
    "        active region on the stack receives that segment's share of energy.\n",
    "        \"\"\"\n",
    "        return self._compute_attribution_core(\n",
    "            ranks_to_metrics=ranks_to_metrics,\n",
    "            instantaneous_metrics=instantaneous_metrics,\n",
    "            duration_threshold=duration_threshold,\n",
    "            inclusive=False,\n",
    "        )\n",
    "\n",
    "    def __str__(self):\n",
    "        return f\"Node(name={self.name}, ranks={[rank.name for rank in self.ranks]})\"\n",
    "\n",
    "class Run:\n",
    "    def __init__(self, path: str, nodes: List[Node] = []):\n",
    "        self.path = path\n",
    "        self.nodes = nodes\n",
    "        \n",
    "    def refresh(self):\n",
    "        return Run(self.path, [node.refresh() for node in self.nodes])\n",
    "\n",
    "    @staticmethod\n",
    "    def from_trace_path(trace_path: str, node_ranks: Dict[str, List[str]]):\n",
    "        # Load the trace dataframes for each node and its ranks\n",
    "        if not os.path.exists(trace_path):\n",
    "            raise FileNotFoundError(f\"Trace file {trace_path} does not exist.\")\n",
    "        # print(f\"Loading trace from {trace_path} for idle period {idle_period_ms} ms and active period {active_period_ms} ms.\")\n",
    "        # trace_to_csv(trace_path, processes, METRICS_TO_TRACK)\n",
    "        # Load the CSV files into dataframes\n",
    "        all_threads = []\n",
    "        for _, ranks in node_ranks.items():\n",
    "            all_threads.extend(ranks)\n",
    "        result = {}\n",
    "        # trace_to_csv(trace_path, all_threads, METRICS_TO_TRACK)\n",
    "        for node, ranks in node_ranks.items():\n",
    "            if not os.path.exists(os.path.join(trace_path, f\"{ranks[0]}_metrics.csv\")):\n",
    "                raise FileNotFoundError(f\"Metrics file for rank {ranks[0]} does not exist in trace path {trace_path}.\")\n",
    "            metrics_df = read_metrics_dataframe(os.path.join(trace_path, f\"{ranks[0]}_metrics.csv\"))\n",
    "            # print(f\"Loaded metrics dataframe from {trace_path}\")\n",
    "            assert metrics_df.columns.tolist() == ['Group', 'Metric Name', 'Time', 'Value'], f\"Metrics DataFrame has incorrect columns, found {metrics_df.columns.tolist()}\"\n",
    "            rank_callgraphs = []\n",
    "            for rank in ranks:\n",
    "                if os.path.exists(os.path.join(trace_path, f\"{rank}_Master_thread_callgraph.csv\")):\n",
    "                    call_graph = read_call_graph_dataframe(os.path.join(trace_path, f\"{rank}_Master_thread_callgraph.csv\"))\n",
    "                    assert call_graph.columns.tolist() == ['Thread', 'Group', 'Depth', 'Name', 'Start Time', 'End Time', 'Duration'], f\"Call graph DataFrame has incorrect columns, found {call_graph.columns.tolist()}\"\n",
    "                    # Construct the Hatchet and Thicket graphs\n",
    "                    rank_callgraphs.append(Rank(node, rank, call_graph))\n",
    "                else:\n",
    "                    raise FileNotFoundError(f\"Call graph file for rank {rank} does not exist in trace path {trace_path}.\")\n",
    "            result[node] = Node(node, metrics_df, rank_callgraphs)\n",
    "        return Run(trace_path, list(result.values()))\n",
    "\n",
    "    def compute_attribution(self, ranks_to_metrics: Dict[str, List[str]]={}, instantaneous_metrics=[]):\n",
    "        attributions = {}\n",
    "        for node in self.nodes:\n",
    "            # print(f\"Computing attribution for node {node.name}...\")\n",
    "            attributions[node.name] = node.compute_attribution(ranks_to_metrics, instantaneous_metrics)\n",
    "        return attributions\n",
    "    \n",
    "    def compute_attribution_exclusive(self, ranks_to_metrics: Dict[str, List[str]] = {}, instantaneous_metrics = []):\n",
    "        attributions = {}\n",
    "        for node in self.nodes:\n",
    "            # print(f\"Computing EXCLUSIVE attribution for node {node.name}...\")\n",
    "            attributions[node.name] = node.compute_attribution_exclusive(\n",
    "                ranks_to_metrics,\n",
    "                instantaneous_metrics,\n",
    "            )\n",
    "        return attributions\n",
    "    \n",
    "    def get_metric_samples(self, node_name: str, metric_name: str) -> pd.DataFrame:\n",
    "        for node in self.nodes:\n",
    "            if node.name == node_name:\n",
    "                return node.get_metric_samples(metric_name)\n",
    "        raise ValueError(f\"Node {node_name} not found in run.\")\n",
    "    \n",
    "    def to_thicket(self, **metadata) -> tt.Thicket:\n",
    "        thickets = []\n",
    "        \n",
    "        for node in self.nodes:\n",
    "            # self.get_metric_samples(node)\n",
    "            metric_functions = {}\n",
    "            for metric_name in node.get_metric_names():\n",
    "                metric_functions[metric_name] = sampled_to_continuous(node.get_metric_samples(metric_name))\n",
    "            \n",
    "            for rank in node.ranks:\n",
    "                print(f\"Creating Thicket for node {node.name}, rank {rank.name}...\")\n",
    "                dag = create_hatchet_dag(rank.call_graph, metric_functions)\n",
    "                tf = tt.Thicket.from_literal(dag)\n",
    "                # tf = tt.Thicket.from_literal(dag)\n",
    "                print(\"Thicket created from DAG.\")\n",
    "                tf.metadata = pd.DataFrame.from_dict(tf.profile_mapping, orient=\"index\")\n",
    "                tf.metadata['rank'] = rank.name\n",
    "                tf.metadata['node'] = node.name\n",
    "                tf.metadata = tf.metadata.assign(**metadata)\n",
    "                tf.metadata.index.name = tf.dataframe.index.names[1]\n",
    "        \n",
    "                thickets.append(tf)\n",
    "        \n",
    "        print(f\"Total thickets created: {len(thickets)}\")\n",
    "        if thickets:\n",
    "            print(\"Combining thickets...\")\n",
    "            return tt.Thicket.concat_thickets(thickets, disable_tqdm=True)\n",
    "        raise ValueError(\"No thickets were created from the run.\")\n",
    "        \n",
    "    def __str__(self):\n",
    "        return f\"Run(path={self.path}, nodes={[str(node) for node in self.nodes]})\"\n",
    "\n",
    "class Ensemble:\n",
    "    def __init__(self, runs: List[Run]):\n",
    "        self.runs = runs\n",
    "        self.attributions = {}\n",
    "        self.attributions_exclusive = {}\n",
    "    \n",
    "    @staticmethod\n",
    "    def from_trace_paths(trace_paths: List[str], node_ranks: Dict[str, List[str]]):\n",
    "        return ensemble_from_trace_path_helper(trace_paths, node_ranks)\n",
    " \n",
    "    def refresh(self):\n",
    "        return Ensemble([run.refresh() for run in self.runs])\n",
    "    \n",
    "    def compute_attribution(self, ranks_to_metrics: Dict[str, List[str]]={}, instantaneous_metrics=[]):\n",
    "        if self.attributions != {}:\n",
    "            print(\"Attributions already computed, returning cached results.\")\n",
    "            return self.attributions\n",
    "        \n",
    "        for run in self.runs:\n",
    "            self.attributions[os.path.basename(os.path.dirname(run.path))] = run.compute_attribution(ranks_to_metrics, instantaneous_metrics)\n",
    "        return self.attributions\n",
    " \n",
    "    def compute_attribution_across_runs(self, ranks_to_metrics: Dict[str, List[str]] = {}, instantaneous_metrics = []):\n",
    "        attributions = self.compute_attribution(ranks_to_metrics, instantaneous_metrics)\n",
    "        if not attributions:\n",
    "            return pd.DataFrame(columns=['Run', 'Node', 'Code Region', 'Metric', 'Value'])\n",
    "\n",
    "        records = []\n",
    "        for run_name, node_attribs in attributions.items():\n",
    "            for node_name, df in node_attribs.items():\n",
    "                if df is None or df.empty: \n",
    "                    continue\n",
    "                code = df['Code Region'].to_numpy()\n",
    "                for metric in df.columns:\n",
    "                    if metric == 'Code Region': \n",
    "                        continue\n",
    "                    vals = df[metric].to_numpy()\n",
    "                    # Extend in bulk (no per-iteration concatenations)\n",
    "                    records.extend(\n",
    "                        {'Run': run_name, 'Node': node_name, 'Code Region': c, 'Metric': metric, 'Value': v}\n",
    "                        for c, v in zip(code, vals)\n",
    "                    )\n",
    "        return pd.DataFrame.from_records(records)\n",
    "\n",
    "    def compute_attribution_exclusive(self, ranks_to_metrics: Dict[str, List[str]] = {}, instantaneous_metrics = []):\n",
    "        if self.attributions_exclusive != {}:\n",
    "            print(\"Exclusive attributions already computed, returning cached results.\")\n",
    "            return self.attributions_exclusive\n",
    "        \n",
    "        for run in self.runs:\n",
    "            run_name = os.path.basename(os.path.dirname(run.path))\n",
    "            print(f\"Computing EXCLUSIVE attribution for run {run_name}...\")\n",
    "            self.attributions_exclusive[run_name] = run.compute_attribution_exclusive(\n",
    "                ranks_to_metrics,\n",
    "                instantaneous_metrics,\n",
    "            )\n",
    "        return self.attributions_exclusive\n",
    "\n",
    "    def compute_attribution_exclusive_across_runs(\n",
    "        self,\n",
    "        ranks_to_metrics: Dict[str, List[str]] = {},\n",
    "        instantaneous_metrics = [],\n",
    "    ):\n",
    "        attributions = self.compute_attribution_exclusive(ranks_to_metrics, instantaneous_metrics)\n",
    "        if not attributions:\n",
    "            return pd.DataFrame(columns=['Run', 'Node', 'Code Region', 'Metric', 'Value'])\n",
    "\n",
    "        records = []\n",
    "        for run_name, node_attribs in attributions.items():\n",
    "            for node_name, df in node_attribs.items():\n",
    "                if df is None or df.empty:\n",
    "                    continue\n",
    "                code = df['Code Region'].to_numpy()\n",
    "                for metric in df.columns:\n",
    "                    if metric == 'Code Region':\n",
    "                        continue\n",
    "                    vals = df[metric].to_numpy()\n",
    "                    records.extend(\n",
    "                        {\n",
    "                            'Run': run_name,\n",
    "                            'Node': node_name,\n",
    "                            'Code Region': c,\n",
    "                            'Metric': metric,\n",
    "                            'Value': v,\n",
    "                        }\n",
    "                        for c, v in zip(code, vals)\n",
    "                    )\n",
    "        return pd.DataFrame.from_records(records)\n",
    "    \n",
    "    def to_thicket(self) -> tt.Thicket:\n",
    "        thickets = []\n",
    "        for run in self.runs:\n",
    "            name = os.path.basename(os.path.dirname(run.path))\n",
    "            print(f\"Creating Thicket for run {run.path} ({name})...\")\n",
    "            tf = run.to_thicket(trace=run.path, run=name)\n",
    "            thickets.append(tf)\n",
    "        if thickets:\n",
    "            return tt.Thicket.concat_thickets(thickets, disable_tqdm=True)\n",
    "        else:\n",
    "            return tt.Thicket()\n",
    "\n",
    "\n",
    "_global_executor = None\n",
    "\n",
    "def get_threadpool(max_workers=None):\n",
    "    global _global_executor\n",
    "    if _global_executor is None:\n",
    "        max_workers = max_workers or min(os.cpu_count(), 256)\n",
    "        # print(f\"Initializing global thread pool with {max_workers} workers...\\r\")\n",
    "        _global_executor = ProcessPoolExecutor(max_workers=max_workers)\n",
    "    return _global_executor\n",
    "\n",
    "def run_from_trace_path_helper(args) -> Run:\n",
    "    path, node_ranks = args\n",
    "    return Run.from_trace_path(path, node_ranks)\n",
    "\n",
    "def _load_metrics(args):\n",
    "    path, node, mpath = args\n",
    "    if not os.path.exists(mpath):\n",
    "        raise FileNotFoundError(f\"Metrics file {mpath} missing.\")\n",
    "    df = read_metrics_dataframe(mpath)\n",
    "    assert df.columns.tolist() == ['Group', 'Metric Name', 'Time', 'Value'], f\"Bad metrics columns in {mpath}: {df.columns.tolist()}\"\n",
    "    return path, node, df\n",
    "\n",
    "def _load_callgraph(args):\n",
    "    path, node, rank, cpath = args\n",
    "    if not os.path.exists(cpath):\n",
    "        raise FileNotFoundError(f\"Call graph file {cpath} missing.\")\n",
    "    df = read_call_graph_dataframe(cpath)\n",
    "    assert df.columns.tolist() == ['Thread', 'Group', 'Depth', 'Name', 'Start Time', 'End Time', 'Duration'], f\"Bad callgraph columns in {cpath}: {df.columns.tolist()}\"\n",
    "    return path, node, rank, df\n",
    "\n",
    "def ensemble_from_trace_path_helper(trace_paths: List[str], node_ranks: Dict[str, List[str]]) -> Ensemble:\n",
    "    # Flatten all work across all runs (trace_to_csv + metrics + callgraphs)\n",
    "    for path in trace_paths:\n",
    "        if not os.path.exists(path):\n",
    "            raise FileNotFoundError(f\"Trace path {path} does not exist.\")\n",
    "    all_threads = [rank for ranks in node_ranks.values() for rank in ranks]\n",
    "\n",
    "    exec_csv = get_threadpool(128)\n",
    "\n",
    "    # Build tasks for metrics (one per (run,node)) and callgraphs (one per (run,node,rank))\n",
    "    metrics_tasks = []\n",
    "    callgraph_tasks = []\n",
    "    for path in trace_paths:\n",
    "        for node, ranks in node_ranks.items():\n",
    "            print(f\"Preparing trace data for run {path}, node {node}...\")\n",
    "            print(f'Metrics will be loaded from: {os.path.join(path, f\"{ranks[0]}_metrics.csv\")}')\n",
    "            metrics_tasks.append((path, node, os.path.join(path, f\"{ranks[0]}_metrics.csv\")))\n",
    "            for rank in ranks:\n",
    "                callgraph_tasks.append((path, node, rank, os.path.join(path, f\"{rank}_Master_thread_callgraph.csv\")))\n",
    "\n",
    "    exec_load = get_threadpool()\n",
    "    fut_metrics = [exec_load.submit(_load_metrics, t) for t in metrics_tasks]\n",
    "    fut_callgraphs = [exec_load.submit(_load_callgraph, t) for t in callgraph_tasks]\n",
    "\n",
    "    metrics_by_run_node = defaultdict(dict)\n",
    "    for f in tqdm(as_completed(fut_metrics), total=len(fut_metrics), desc=\"Loading metrics\", leave=False):\n",
    "        path, node, dfm = f.result()\n",
    "        metrics_by_run_node[(path, node)] = dfm\n",
    "\n",
    "    callgraph_by_run_node_rank = defaultdict(dict)\n",
    "    for f in tqdm(as_completed(fut_callgraphs), total=len(fut_callgraphs), desc=\"Loading callgraphs\", leave=False):\n",
    "        path, node, rank, dfc = f.result()\n",
    "        callgraph_by_run_node_rank[(path, node, rank)] = dfc\n",
    "\n",
    "    # Assemble Run objects\n",
    "    runs = []\n",
    "    for path in trace_paths:\n",
    "        node_objs = []\n",
    "        for node, ranks in node_ranks.items():\n",
    "            mdf = metrics_by_run_node.get((path, node))\n",
    "            if mdf is None:\n",
    "                raise ValueError(f\"Missing metrics for run {path}, node {node}\")\n",
    "            rank_objs = []\n",
    "            for rank in ranks:\n",
    "                cdf = callgraph_by_run_node_rank.get((path, node, rank))\n",
    "                if cdf is None:\n",
    "                    raise ValueError(f\"Missing callgraph for run {path}, node {node}, rank {rank}\")\n",
    "                rank_objs.append(Rank(node, rank, cdf))\n",
    "            node_objs.append(Node(node, mdf, rank_objs))\n",
    "        runs.append(Run(path, node_objs))\n",
    "    \n",
    "    return Ensemble(runs)\n",
    "\n",
    "# First, replace all function names with the following function:\n",
    "def replace_prefix_and_suffix(name: str) -> str:\n",
    "    # name = name.replace('HPLMXP_', 'HPL_')  # Replace HPLMXP with HPL\n",
    "    if '<' in name:\n",
    "        name = name.split('<')[0]\n",
    "    if '(' in name:\n",
    "        name = name.split('(')[0]\n",
    "    return name.strip()\n",
    "\n",
    "\n",
    "def take_alphanumeric(s):\n",
    "    # Take up to the first non-alphanumeric characters\n",
    "    match = re.match(r'^[~<, >:a-zA-Z0-9_]+', s)\n",
    "    return replace_prefix_and_suffix(match.group(0) if match else s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a6d8eda",
   "metadata": {},
   "source": [
    "# Begin Analysis!\n",
    "Load in the rocHPL data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3803cd6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of locations: 45\n",
      "Number of readers: 45\n",
      "Event Summary: total events read across readers = 17056588 in 2.83616 seconds\n",
      "Writing: ./frontier-1-node-single-HPL-run/MPI Rank 7_Master_thread_callgraph.csv\n",
      "Writing: ./frontier-1-node-single-HPL-run/MPI Rank 3_Master_thread_callgraph.csv\n",
      "Writing: ./frontier-1-node-single-HPL-run/MPI Rank 5_Master_thread_callgraph.csv\n",
      "Writing: ./frontier-1-node-single-HPL-run/MPI Rank 6_Master_thread_callgraph.csv\n",
      "Writing: ./frontier-1-node-single-HPL-run/MPI Rank 4_Master_thread_callgraph.csv\n",
      "Writing: ./frontier-1-node-single-HPL-run/MPI Rank 2_Master_thread_callgraph.csv\n",
      "Writing: ./frontier-1-node-single-HPL-run/MPI Rank 0_Master_thread_callgraph.csv\n",
      "Writing: ./frontier-1-node-single-HPL-run/MPI Rank 0_UnknownLocation_callgraph.csv\n",
      "Writing: ./frontier-1-node-single-HPL-run/MPI Rank 1_Master_thread_callgraph.csv\n",
      "Writing: ./frontier-1-node-single-HPL-run/MPI Rank 0_metrics.csv\n",
      "Writing: ./frontier-1-node-single-HPL-run/MPI Rank 3_metrics.csv\n",
      "Writing: ./frontier-1-node-single-HPL-run/MPI Rank 5_metrics.csv\n",
      "Writing: ./frontier-1-node-single-HPL-run/MPI Rank 4_metrics.csv\n",
      "Writing: ./frontier-1-node-single-HPL-run/MPI Rank 2_metrics.csv\n",
      "Writing: ./frontier-1-node-single-HPL-run/MPI Rank 1_metrics.csv\n",
      "Writing: ./frontier-1-node-single-HPL-run/MPI Rank 6_metrics.csv\n",
      "Writing: ./frontier-1-node-single-HPL-run/MPI Rank 7_metrics.csv\n",
      "Total time: 9.85554 seconds\n",
      "Preparing trace data for run ./frontier-1-node-single-HPL-run, node node0...\n",
      "Metrics will be loaded from: ./frontier-1-node-single-HPL-run/MPI Rank 0_metrics.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                 \r"
     ]
    }
   ],
   "source": [
    "HPL_NODE_RANKS = {\n",
    "    'node0': ['MPI Rank 0', 'MPI Rank 1', 'MPI Rank 2', 'MPI Rank 3', 'MPI Rank 4', 'MPI Rank 5', 'MPI Rank 6', 'MPI Rank 7'],\n",
    "}\n",
    "start_convert = time.time()\n",
    "os.system(\"LD_LIBRARY_PATH=\\\"/opt/rocm-6.4.1/lib:/lustre/orion/csc688/world-shared/scorep-amd/install/lib:$LD_LIBRARY_PATH\\\" CHPL_RT_NUM_THREADS_PER_LOCALE=64 ./fast-OTF2/trace_to_csv_parallel_real -nl1 --trace ./frontier-1-node-single-HPL-run/traces.otf2 --outputDir ./frontier-1-node-single-HPL-run\")\n",
    "start_load = time.time()\n",
    "ens = Ensemble.from_trace_paths(['./frontier-1-node-single-HPL-run'], HPL_NODE_RANKS)\n",
    "end = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "94b19720",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trace conversion finished in 10.54 seconds.\n",
      "Loaded Ensemble in 1.57 seconds.\n",
      "Converted and loaded ensemble in 12.11 seconds.\n"
     ]
    }
   ],
   "source": [
    "print(f\"Trace conversion finished in {start_load - start_convert:.2f} seconds.\")\n",
    "print(f\"Loaded Ensemble in {end - start_load:.2f} seconds.\")\n",
    "print(f\"Converted and loaded ensemble in {end - start_convert:.2f} seconds.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "67d48634",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of locations: 816\n",
      "Number of readers: 64\n",
      "Event Summary: total events read across readers = 848806084 in 104.969 seconds\n",
      "Writing: ./frontier-16-node-single-HPL-run/MPI Rank 17_Master_thread_callgraph.csv\n",
      "Writing: ./frontier-16-node-single-HPL-run/MPI Rank 11_Master_thread_callgraph.csv\n",
      "Writing: ./frontier-16-node-single-HPL-run/MPI Rank 127_Master_thread_callgraph.csv\n",
      "Writing: ./frontier-16-node-single-HPL-run/MPI Rank 121_Master_thread_callgraph.csv\n",
      "Writing: ./frontier-16-node-single-HPL-run/MPI Rank 102_Master_thread_callgraph.csv\n",
      "Writing: ./frontier-16-node-single-HPL-run/MPI Rank 107_Master_thread_callgraph.csv\n",
      "Writing: ./frontier-16-node-single-HPL-run/MPI Rank 123_Master_thread_callgraph.csv\n",
      "Writing: ./frontier-16-node-single-HPL-run/MPI Rank 106_Master_thread_callgraph.csv\n",
      "Writing: ./frontier-16-node-single-HPL-run/MPI Rank 103_Master_thread_callgraph.csv\n",
      "Writing: ./frontier-16-node-single-HPL-run/MPI Rank 105_Master_thread_callgraph.csv\n",
      "Writing: ./frontier-16-node-single-HPL-run/MPI Rank 109_Master_thread_callgraph.csv\n",
      "Writing: ./frontier-16-node-single-HPL-run/MPI Rank 101_Master_thread_callgraph.csv\n",
      "Writing: ./frontier-16-node-single-HPL-run/MPI Rank 122_Master_thread_callgraph.csv\n",
      "Writing: ./frontier-16-node-single-HPL-run/MPI Rank 89_Master_thread_callgraph.csv\n",
      "Writing: ./frontier-16-node-single-HPL-run/MPI Rank 100_Master_thread_callgraph.csv\n",
      "Writing: ./frontier-16-node-single-HPL-run/MPI Rank 108_Master_thread_callgraph.csv\n",
      "Writing: ./frontier-16-node-single-HPL-run/MPI Rank 86_Master_thread_callgraph.csv\n",
      "Writing: ./frontier-16-node-single-HPL-run/MPI Rank 87_Master_thread_callgraph.csv\n",
      "Writing: ./frontier-16-node-single-HPL-run/MPI Rank 13_Master_thread_callgraph.csv\n",
      "Writing: ./frontier-16-node-single-HPL-run/MPI Rank 14_Master_thread_callgraph.csv\n",
      "Writing: ./frontier-16-node-single-HPL-run/MPI Rank 12_Master_thread_callgraph.csv\n",
      "Writing: ./frontier-16-node-single-HPL-run/MPI Rank 15_Master_thread_callgraph.csv\n",
      "Writing: ./frontier-16-node-single-HPL-run/MPI Rank 10_Master_thread_callgraph.csv\n",
      "Writing: ./frontier-16-node-single-HPL-run/MPI Rank 124_Master_thread_callgraph.csv\n",
      "Writing: ./frontier-16-node-single-HPL-run/MPI Rank 114_Master_thread_callgraph.csv\n",
      "Writing: ./frontier-16-node-single-HPL-run/MPI Rank 118_Master_thread_callgraph.csv\n",
      "Writing: ./frontier-16-node-single-HPL-run/MPI Rank 76_Master_thread_callgraph.csv\n",
      "Writing: ./frontier-16-node-single-HPL-run/MPI Rank 46_Master_thread_callgraph.csv\n",
      "Writing: ./frontier-16-node-single-HPL-run/MPI Rank 125_Master_thread_callgraph.csv\n",
      "Writing: ./frontier-16-node-single-HPL-run/MPI Rank 70_Master_thread_callgraph.csv\n",
      "Writing: ./frontier-16-node-single-HPL-run/MPI Rank 19_Master_thread_callgraph.csv\n",
      "Writing: ./frontier-16-node-single-HPL-run/MPI Rank 18_Master_thread_callgraph.csv\n",
      "Writing: ./frontier-16-node-single-HPL-run/MPI Rank 126_Master_thread_callgraph.csv\n",
      "Writing: ./frontier-16-node-single-HPL-run/MPI Rank 120_UnknownLocation_callgraph.csv\n",
      "Writing: ./frontier-16-node-single-HPL-run/MPI Rank 54_Master_thread_callgraph.csv\n",
      "Writing: ./frontier-16-node-single-HPL-run/MPI Rank 55_Master_thread_callgraph.csv\n",
      "Writing: ./frontier-16-node-single-HPL-run/MPI Rank 58_Master_thread_callgraph.csv\n",
      "Writing: ./frontier-16-node-single-HPL-run/MPI Rank 25_Master_thread_callgraph.csv\n",
      "Writing: ./frontier-16-node-single-HPL-run/MPI Rank 28_Master_thread_callgraph.csv\n",
      "Writing: ./frontier-16-node-single-HPL-run/MPI Rank 84_Master_thread_callgraph.csv\n",
      "Writing: ./frontier-16-node-single-HPL-run/MPI Rank 95_Master_thread_callgraph.csv\n",
      "Writing: ./frontier-16-node-single-HPL-run/MPI Rank 39_Master_thread_callgraph.csv\n",
      "Writing: ./frontier-16-node-single-HPL-run/MPI Rank 34_Master_thread_callgraph.csv\n",
      "Writing: ./frontier-16-node-single-HPL-run/MPI Rank 113_Master_thread_callgraph.csv\n",
      "Writing: ./frontier-16-node-single-HPL-run/MPI Rank 23_Master_thread_callgraph.csv\n",
      "Writing: ./frontier-16-node-single-HPL-run/MPI Rank 57_Master_thread_callgraph.csv\n",
      "Writing: ./frontier-16-node-single-HPL-run/MPI Rank 81_Master_thread_callgraph.csv\n",
      "Writing: ./frontier-16-node-single-HPL-run/MPI Rank 83_Master_thread_callgraph.csv\n",
      "Writing: ./frontier-16-node-single-HPL-run/MPI Rank 98_Master_thread_callgraph.csv\n",
      "Writing: ./frontier-16-node-single-HPL-run/MPI Rank 82_Master_thread_callgraph.csv\n",
      "Writing: ./frontier-16-node-single-HPL-run/MPI Rank 85_Master_thread_callgraph.csv\n",
      "Writing: ./frontier-16-node-single-HPL-run/MPI Rank 51_Master_thread_callgraph.csv\n",
      "Writing: ./frontier-16-node-single-HPL-run/MPI Rank 116_Master_thread_callgraph.csv\n",
      "Writing: ./frontier-16-node-single-HPL-run/MPI Rank 35_Master_thread_callgraph.csv\n",
      "Writing: ./frontier-16-node-single-HPL-run/MPI Rank 38_Master_thread_callgraph.csv\n",
      "Writing: ./frontier-16-node-single-HPL-run/MPI Rank 90_Master_thread_callgraph.csv\n",
      "Writing: ./frontier-16-node-single-HPL-run/MPI Rank 26_Master_thread_callgraph.csv\n",
      "Writing: ./frontier-16-node-single-HPL-run/MPI Rank 93_Master_thread_callgraph.csv\n",
      "Writing: ./frontier-16-node-single-HPL-run/MPI Rank 61_Master_thread_callgraph.csv\n",
      "Writing: ./frontier-16-node-single-HPL-run/MPI Rank 62_Master_thread_callgraph.csv\n",
      "Writing: ./frontier-16-node-single-HPL-run/MPI Rank 31_Master_thread_callgraph.csv\n",
      "Writing: ./frontier-16-node-single-HPL-run/MPI Rank 36_Master_thread_callgraph.csv\n",
      "Writing: ./frontier-16-node-single-HPL-run/MPI Rank 63_Master_thread_callgraph.csv\n",
      "Writing: ./frontier-16-node-single-HPL-run/MPI Rank 21_Master_thread_callgraph.csv\n",
      "Writing: ./frontier-16-node-single-HPL-run/MPI Rank 104_UnknownLocation_callgraph.csv\n",
      "Writing: ./frontier-16-node-single-HPL-run/MPI Rank 88_UnknownLocation_callgraph.csv\n",
      "Writing: ./frontier-16-node-single-HPL-run/MPI Rank 80_UnknownLocation_callgraph.csv\n",
      "Writing: ./frontier-16-node-single-HPL-run/MPI Rank 96_UnknownLocation_callgraph.csv\n",
      "Writing: ./frontier-16-node-single-HPL-run/MPI Rank 112_UnknownLocation_callgraph.csv\n",
      "Writing: ./frontier-16-node-single-HPL-run/MPI Rank 67_Master_thread_callgraph.csv\n",
      "Writing: ./frontier-16-node-single-HPL-run/MPI Rank 33_Master_thread_callgraph.csv\n",
      "Writing: ./frontier-16-node-single-HPL-run/MPI Rank 65_Master_thread_callgraph.csv\n",
      "Writing: ./frontier-16-node-single-HPL-run/MPI Rank 92_Master_thread_callgraph.csv\n",
      "Writing: ./frontier-16-node-single-HPL-run/MPI Rank 60_Master_thread_callgraph.csv\n",
      "Writing: ./frontier-16-node-single-HPL-run/MPI Rank 119_Master_thread_callgraph.csv\n",
      "Writing: ./frontier-16-node-single-HPL-run/MPI Rank 99_Master_thread_callgraph.csv\n",
      "Writing: ./frontier-16-node-single-HPL-run/MPI Rank 97_Master_thread_callgraph.csv\n",
      "Writing: ./frontier-16-node-single-HPL-run/MPI Rank 20_Master_thread_callgraph.csv\n",
      "Writing: ./frontier-16-node-single-HPL-run/MPI Rank 59_Master_thread_callgraph.csv\n",
      "Writing: ./frontier-16-node-single-HPL-run/MPI Rank 94_Master_thread_callgraph.csv\n",
      "Writing: ./frontier-16-node-single-HPL-run/MPI Rank 115_Master_thread_callgraph.csv\n",
      "Writing: ./frontier-16-node-single-HPL-run/MPI Rank 30_Master_thread_callgraph.csv\n",
      "Writing: ./frontier-16-node-single-HPL-run/MPI Rank 110_Master_thread_callgraph.csv\n",
      "Writing: ./frontier-16-node-single-HPL-run/MPI Rank 22_Master_thread_callgraph.csv\n",
      "Writing: ./frontier-16-node-single-HPL-run/MPI Rank 91_Master_thread_callgraph.csv\n",
      "Writing: ./frontier-16-node-single-HPL-run/MPI Rank 29_Master_thread_callgraph.csv\n",
      "Writing: ./frontier-16-node-single-HPL-run/MPI Rank 50_Master_thread_callgraph.csv\n",
      "Writing: ./frontier-16-node-single-HPL-run/MPI Rank 37_Master_thread_callgraph.csv\n",
      "Writing: ./frontier-16-node-single-HPL-run/MPI Rank 117_Master_thread_callgraph.csv\n",
      "Writing: ./frontier-16-node-single-HPL-run/MPI Rank 53_Master_thread_callgraph.csv\n",
      "Writing: ./frontier-16-node-single-HPL-run/MPI Rank 52_Master_thread_callgraph.csv\n",
      "Writing: ./frontier-16-node-single-HPL-run/MPI Rank 27_Master_thread_callgraph.csv\n",
      "Writing: ./frontier-16-node-single-HPL-run/MPI Rank 79_Master_thread_callgraph.csv\n",
      "Writing: ./frontier-16-node-single-HPL-run/MPI Rank 68_Master_thread_callgraph.csv\n",
      "Writing: ./frontier-16-node-single-HPL-run/MPI Rank 4_Master_thread_callgraph.csv\n",
      "Writing: ./frontier-16-node-single-HPL-run/MPI Rank 7_Master_thread_callgraph.csv\n",
      "Writing: ./frontier-16-node-single-HPL-run/MPI Rank 75_Master_thread_callgraph.csv\n",
      "Writing: ./frontier-16-node-single-HPL-run/MPI Rank 6_Master_thread_callgraph.csv\n",
      "Writing: ./frontier-16-node-single-HPL-run/MPI Rank 49_Master_thread_callgraph.csv\n",
      "Writing: ./frontier-16-node-single-HPL-run/MPI Rank 3_Master_thread_callgraph.csv\n",
      "Writing: ./frontier-16-node-single-HPL-run/MPI Rank 45_Master_thread_callgraph.csv\n",
      "Writing: ./frontier-16-node-single-HPL-run/MPI Rank 2_Master_thread_callgraph.csv\n",
      "Writing: ./frontier-16-node-single-HPL-run/MPI Rank 77_Master_thread_callgraph.csv\n",
      "Writing: ./frontier-16-node-single-HPL-run/MPI Rank 43_Master_thread_callgraph.csv\n",
      "Writing: ./frontier-16-node-single-HPL-run/MPI Rank 9_Master_thread_callgraph.csv\n",
      "Writing: ./frontier-16-node-single-HPL-run/MPI Rank 47_Master_thread_callgraph.csv\n",
      "Writing: ./frontier-16-node-single-HPL-run/MPI Rank 111_Master_thread_callgraph.csv\n",
      "Writing: ./frontier-16-node-single-HPL-run/MPI Rank 74_Master_thread_callgraph.csv\n",
      "Writing: ./frontier-16-node-single-HPL-run/MPI Rank 44_Master_thread_callgraph.csv\n",
      "Writing: ./frontier-16-node-single-HPL-run/MPI Rank 71_Master_thread_callgraph.csv\n",
      "Writing: ./frontier-16-node-single-HPL-run/MPI Rank 42_Master_thread_callgraph.csv\n",
      "Writing: ./frontier-16-node-single-HPL-run/MPI Rank 69_Master_thread_callgraph.csv\n",
      "Writing: ./frontier-16-node-single-HPL-run/MPI Rank 5_Master_thread_callgraph.csv\n",
      "Writing: ./frontier-16-node-single-HPL-run/MPI Rank 78_Master_thread_callgraph.csv\n",
      "Writing: ./frontier-16-node-single-HPL-run/MPI Rank 1_Master_thread_callgraph.csv\n",
      "Writing: ./frontier-16-node-single-HPL-run/MPI Rank 41_Master_thread_callgraph.csv\n",
      "Writing: ./frontier-16-node-single-HPL-run/MPI Rank 120_Master_thread_callgraph.csv\n",
      "Writing: ./frontier-16-node-single-HPL-run/MPI Rank 73_Master_thread_callgraph.csv\n",
      "Writing: ./frontier-16-node-single-HPL-run/MPI Rank 16_UnknownLocation_callgraph.csv\n",
      "Writing: ./frontier-16-node-single-HPL-run/MPI Rank 64_UnknownLocation_callgraph.csv\n",
      "Writing: ./frontier-16-node-single-HPL-run/MPI Rank 0_UnknownLocation_callgraph.csv\n",
      "Writing: ./frontier-16-node-single-HPL-run/MPI Rank 48_UnknownLocation_callgraph.csv\n",
      "Writing: ./frontier-16-node-single-HPL-run/MPI Rank 56_UnknownLocation_callgraph.csv\n",
      "Writing: ./frontier-16-node-single-HPL-run/MPI Rank 24_UnknownLocation_callgraph.csv\n",
      "Writing: ./frontier-16-node-single-HPL-run/MPI Rank 32_UnknownLocation_callgraph.csv\n",
      "Writing: ./frontier-16-node-single-HPL-run/MPI Rank 40_UnknownLocation_callgraph.csv\n",
      "Writing: ./frontier-16-node-single-HPL-run/MPI Rank 8_UnknownLocation_callgraph.csv\n",
      "Writing: ./frontier-16-node-single-HPL-run/MPI Rank 72_UnknownLocation_callgraph.csv\n",
      "Writing: ./frontier-16-node-single-HPL-run/MPI Rank 104_Master_thread_callgraph.csv\n",
      "Writing: ./frontier-16-node-single-HPL-run/MPI Rank 88_Master_thread_callgraph.csv\n",
      "Writing: ./frontier-16-node-single-HPL-run/MPI Rank 80_Master_thread_callgraph.csv\n",
      "Writing: ./frontier-16-node-single-HPL-run/MPI Rank 96_Master_thread_callgraph.csv\n",
      "Writing: ./frontier-16-node-single-HPL-run/MPI Rank 112_Master_thread_callgraph.csv\n",
      "Writing: ./frontier-16-node-single-HPL-run/MPI Rank 66_Master_thread_callgraph.csv\n",
      "Writing: ./frontier-16-node-single-HPL-run/MPI Rank 16_Master_thread_callgraph.csv\n",
      "Writing: ./frontier-16-node-single-HPL-run/MPI Rank 64_Master_thread_callgraph.csv\n",
      "Writing: ./frontier-16-node-single-HPL-run/MPI Rank 0_Master_thread_callgraph.csv\n",
      "Writing: ./frontier-16-node-single-HPL-run/MPI Rank 48_Master_thread_callgraph.csv\n",
      "Writing: ./frontier-16-node-single-HPL-run/MPI Rank 56_Master_thread_callgraph.csv\n",
      "Writing: ./frontier-16-node-single-HPL-run/MPI Rank 24_Master_thread_callgraph.csv\n",
      "Writing: ./frontier-16-node-single-HPL-run/MPI Rank 32_Master_thread_callgraph.csv\n",
      "Writing: ./frontier-16-node-single-HPL-run/MPI Rank 40_Master_thread_callgraph.csv\n",
      "Writing: ./frontier-16-node-single-HPL-run/MPI Rank 8_Master_thread_callgraph.csv\n",
      "Writing: ./frontier-16-node-single-HPL-run/MPI Rank 72_Master_thread_callgraph.csv\n",
      "Writing: ./frontier-16-node-single-HPL-run/MPI Rank 17_metrics.csv\n",
      "Writing: ./frontier-16-node-single-HPL-run/MPI Rank 14_metrics.csv\n",
      "Writing: ./frontier-16-node-single-HPL-run/MPI Rank 10_metrics.csv\n",
      "Writing: ./frontier-16-node-single-HPL-run/MPI Rank 125_metrics.csv\n",
      "Writing: ./frontier-16-node-single-HPL-run/MPI Rank 19_metrics.csv\n",
      "Writing: ./frontier-16-node-single-HPL-run/MPI Rank 18_metrics.csv\n",
      "Writing: ./frontier-16-node-single-HPL-run/MPI Rank 120_metrics.csv\n",
      "Writing: ./frontier-16-node-single-HPL-run/MPI Rank 127_metrics.csv\n",
      "Writing: ./frontier-16-node-single-HPL-run/MPI Rank 15_metrics.csv\n",
      "Writing: ./frontier-16-node-single-HPL-run/MPI Rank 13_metrics.csv\n",
      "Writing: ./frontier-16-node-single-HPL-run/MPI Rank 11_metrics.csv\n",
      "Writing: ./frontier-16-node-single-HPL-run/MPI Rank 117_metrics.csv\n",
      "Writing: ./frontier-16-node-single-HPL-run/MPI Rank 113_metrics.csv\n",
      "Writing: ./frontier-16-node-single-HPL-run/MPI Rank 116_metrics.csv\n",
      "Writing: ./frontier-16-node-single-HPL-run/MPI Rank 23_metrics.csv\n",
      "Writing: ./frontier-16-node-single-HPL-run/MPI Rank 28_metrics.csv\n",
      "Writing: ./frontier-16-node-single-HPL-run/MPI Rank 110_metrics.csv\n",
      "Writing: ./frontier-16-node-single-HPL-run/MPI Rank 111_metrics.csv\n",
      "Writing: ./frontier-16-node-single-HPL-run/MPI Rank 61_metrics.csv\n",
      "Writing: ./frontier-16-node-single-HPL-run/MPI Rank 36_metrics.csv\n",
      "Writing: ./frontier-16-node-single-HPL-run/MPI Rank 63_metrics.csv\n",
      "Writing: ./frontier-16-node-single-HPL-run/MPI Rank 62_metrics.csv\n",
      "Writing: ./frontier-16-node-single-HPL-run/MPI Rank 118_metrics.csv\n",
      "Writing: ./frontier-16-node-single-HPL-run/MPI Rank 31_metrics.csv\n",
      "Writing: ./frontier-16-node-single-HPL-run/MPI Rank 64_metrics.csv\n",
      "Writing: ./frontier-16-node-single-HPL-run/MPI Rank 114_metrics.csv\n",
      "Writing: ./frontier-16-node-single-HPL-run/MPI Rank 53_metrics.csv\n",
      "Writing: ./frontier-16-node-single-HPL-run/MPI Rank 12_metrics.csv\n",
      "Writing: ./frontier-16-node-single-HPL-run/MPI Rank 67_metrics.csv\n",
      "Writing: ./frontier-16-node-single-HPL-run/MPI Rank 126_metrics.csv\n",
      "Writing: ./frontier-16-node-single-HPL-run/MPI Rank 124_metrics.csv\n",
      "Writing: ./frontier-16-node-single-HPL-run/MPI Rank 71_metrics.csv\n",
      "Writing: ./frontier-16-node-single-HPL-run/MPI Rank 81_metrics.csv\n",
      "Writing: ./frontier-16-node-single-HPL-run/MPI Rank 84_metrics.csv\n",
      "Writing: ./frontier-16-node-single-HPL-run/MPI Rank 49_metrics.csv\n",
      "Writing: ./frontier-16-node-single-HPL-run/MPI Rank 51_metrics.csv\n",
      "Writing: ./frontier-16-node-single-HPL-run/MPI Rank 57_metrics.csv\n",
      "Writing: ./frontier-16-node-single-HPL-run/MPI Rank 98_metrics.csv\n",
      "Writing: ./frontier-16-node-single-HPL-run/MPI Rank 54_metrics.csv\n",
      "Writing: ./frontier-16-node-single-HPL-run/MPI Rank 97_metrics.csv\n",
      "Writing: ./frontier-16-node-single-HPL-run/MPI Rank 27_metrics.csv\n",
      "Writing: ./frontier-16-node-single-HPL-run/MPI Rank 91_metrics.csv\n",
      "Writing: ./frontier-16-node-single-HPL-run/MPI Rank 93_metrics.csv\n",
      "Writing: ./frontier-16-node-single-HPL-run/MPI Rank 92_metrics.csv\n",
      "Writing: ./frontier-16-node-single-HPL-run/MPI Rank 58_metrics.csv\n",
      "Writing: ./frontier-16-node-single-HPL-run/MPI Rank 59_metrics.csv\n",
      "Writing: ./frontier-16-node-single-HPL-run/MPI Rank 65_metrics.csv\n",
      "Writing: ./frontier-16-node-single-HPL-run/MPI Rank 104_metrics.csv\n",
      "Writing: ./frontier-16-node-single-HPL-run/MPI Rank 46_metrics.csv\n",
      "Writing: ./frontier-16-node-single-HPL-run/MPI Rank 83_metrics.csv\n",
      "Writing: ./frontier-16-node-single-HPL-run/MPI Rank 42_metrics.csv\n",
      "Writing: ./frontier-16-node-single-HPL-run/MPI Rank 74_metrics.csv\n",
      "Writing: ./frontier-16-node-single-HPL-run/MPI Rank 75_metrics.csv\n",
      "Writing: ./frontier-16-node-single-HPL-run/MPI Rank 41_metrics.csv\n",
      "Writing: ./frontier-16-node-single-HPL-run/MPI Rank 100_metrics.csv\n",
      "Writing: ./frontier-16-node-single-HPL-run/MPI Rank 78_metrics.csv\n",
      "Writing: ./frontier-16-node-single-HPL-run/MPI Rank 77_metrics.csv\n",
      "Writing: ./frontier-16-node-single-HPL-run/MPI Rank 40_metrics.csv\n",
      "Writing: ./frontier-16-node-single-HPL-run/MPI Rank 79_metrics.csv\n",
      "Writing: ./frontier-16-node-single-HPL-run/MPI Rank 86_metrics.csv\n",
      "Writing: ./frontier-16-node-single-HPL-run/MPI Rank 73_metrics.csv\n",
      "Writing: ./frontier-16-node-single-HPL-run/MPI Rank 45_metrics.csv\n",
      "Writing: ./frontier-16-node-single-HPL-run/MPI Rank 66_metrics.csv\n",
      "Writing: ./frontier-16-node-single-HPL-run/MPI Rank 72_metrics.csv\n",
      "Writing: ./frontier-16-node-single-HPL-run/MPI Rank 47_metrics.csv\n",
      "Writing: ./frontier-16-node-single-HPL-run/MPI Rank 22_metrics.csv\n",
      "Writing: ./frontier-16-node-single-HPL-run/MPI Rank 69_metrics.csv\n",
      "Writing: ./frontier-16-node-single-HPL-run/MPI Rank 106_metrics.csv\n",
      "Writing: ./frontier-16-node-single-HPL-run/MPI Rank 5_metrics.csv\n",
      "Writing: ./frontier-16-node-single-HPL-run/MPI Rank 0_metrics.csv\n",
      "Writing: ./frontier-16-node-single-HPL-run/MPI Rank 2_metrics.csv\n",
      "Writing: ./frontier-16-node-single-HPL-run/MPI Rank 3_metrics.csv\n",
      "Writing: ./frontier-16-node-single-HPL-run/MPI Rank 8_metrics.csv\n",
      "Writing: ./frontier-16-node-single-HPL-run/MPI Rank 4_metrics.csv\n",
      "Writing: ./frontier-16-node-single-HPL-run/MPI Rank 6_metrics.csv\n",
      "Writing: ./frontier-16-node-single-HPL-run/MPI Rank 68_metrics.csv\n",
      "Writing: ./frontier-16-node-single-HPL-run/MPI Rank 29_metrics.csv\n",
      "Writing: ./frontier-16-node-single-HPL-run/MPI Rank 20_metrics.csv\n",
      "Writing: ./frontier-16-node-single-HPL-run/MPI Rank 21_metrics.csv\n",
      "Writing: ./frontier-16-node-single-HPL-run/MPI Rank 115_metrics.csv\n",
      "Writing: ./frontier-16-node-single-HPL-run/MPI Rank 112_metrics.csv\n",
      "Writing: ./frontier-16-node-single-HPL-run/MPI Rank 35_metrics.csv\n",
      "Writing: ./frontier-16-node-single-HPL-run/MPI Rank 34_metrics.csv\n",
      "Writing: ./frontier-16-node-single-HPL-run/MPI Rank 119_metrics.csv\n",
      "Writing: ./frontier-16-node-single-HPL-run/MPI Rank 30_metrics.csv\n",
      "Writing: ./frontier-16-node-single-HPL-run/MPI Rank 32_metrics.csv\n",
      "Writing: ./frontier-16-node-single-HPL-run/MPI Rank 33_metrics.csv\n",
      "Writing: ./frontier-16-node-single-HPL-run/MPI Rank 37_metrics.csv\n",
      "Writing: ./frontier-16-node-single-HPL-run/MPI Rank 60_metrics.csv\n",
      "Writing: ./frontier-16-node-single-HPL-run/MPI Rank 16_metrics.csv\n",
      "Writing: ./frontier-16-node-single-HPL-run/MPI Rank 7_metrics.csv\n",
      "Writing: ./frontier-16-node-single-HPL-run/MPI Rank 1_metrics.csv\n",
      "Writing: ./frontier-16-node-single-HPL-run/MPI Rank 9_metrics.csv\n",
      "Writing: ./frontier-16-node-single-HPL-run/MPI Rank 121_metrics.csv\n",
      "Writing: ./frontier-16-node-single-HPL-run/MPI Rank 50_metrics.csv\n",
      "Writing: ./frontier-16-node-single-HPL-run/MPI Rank 82_metrics.csv\n",
      "Writing: ./frontier-16-node-single-HPL-run/MPI Rank 99_metrics.csv\n",
      "Writing: ./frontier-16-node-single-HPL-run/MPI Rank 48_metrics.csv\n",
      "Writing: ./frontier-16-node-single-HPL-run/MPI Rank 85_metrics.csv\n",
      "Writing: ./frontier-16-node-single-HPL-run/MPI Rank 80_metrics.csv\n",
      "Writing: ./frontier-16-node-single-HPL-run/MPI Rank 56_metrics.csv\n",
      "Writing: ./frontier-16-node-single-HPL-run/MPI Rank 96_metrics.csv\n",
      "Writing: ./frontier-16-node-single-HPL-run/MPI Rank 55_metrics.csv\n",
      "Writing: ./frontier-16-node-single-HPL-run/MPI Rank 90_metrics.csv\n",
      "Writing: ./frontier-16-node-single-HPL-run/MPI Rank 26_metrics.csv\n",
      "Writing: ./frontier-16-node-single-HPL-run/MPI Rank 94_metrics.csv\n",
      "Writing: ./frontier-16-node-single-HPL-run/MPI Rank 25_metrics.csv\n",
      "Writing: ./frontier-16-node-single-HPL-run/MPI Rank 24_metrics.csv\n",
      "Writing: ./frontier-16-node-single-HPL-run/MPI Rank 95_metrics.csv\n",
      "Writing: ./frontier-16-node-single-HPL-run/MPI Rank 38_metrics.csv\n",
      "Writing: ./frontier-16-node-single-HPL-run/MPI Rank 88_metrics.csv\n",
      "Writing: ./frontier-16-node-single-HPL-run/MPI Rank 52_metrics.csv\n",
      "Writing: ./frontier-16-node-single-HPL-run/MPI Rank 101_metrics.csv\n",
      "Writing: ./frontier-16-node-single-HPL-run/MPI Rank 107_metrics.csv\n",
      "Writing: ./frontier-16-node-single-HPL-run/MPI Rank 102_metrics.csv\n",
      "Writing: ./frontier-16-node-single-HPL-run/MPI Rank 105_metrics.csv\n",
      "Writing: ./frontier-16-node-single-HPL-run/MPI Rank 43_metrics.csv\n",
      "Writing: ./frontier-16-node-single-HPL-run/MPI Rank 108_metrics.csv\n",
      "Writing: ./frontier-16-node-single-HPL-run/MPI Rank 103_metrics.csv\n",
      "Writing: ./frontier-16-node-single-HPL-run/MPI Rank 44_metrics.csv\n",
      "Writing: ./frontier-16-node-single-HPL-run/MPI Rank 123_metrics.csv\n",
      "Writing: ./frontier-16-node-single-HPL-run/MPI Rank 87_metrics.csv\n",
      "Writing: ./frontier-16-node-single-HPL-run/MPI Rank 89_metrics.csv\n",
      "Writing: ./frontier-16-node-single-HPL-run/MPI Rank 39_metrics.csv\n",
      "Writing: ./frontier-16-node-single-HPL-run/MPI Rank 76_metrics.csv\n",
      "Writing: ./frontier-16-node-single-HPL-run/MPI Rank 109_metrics.csv\n",
      "Writing: ./frontier-16-node-single-HPL-run/MPI Rank 70_metrics.csv\n",
      "Writing: ./frontier-16-node-single-HPL-run/MPI Rank 122_metrics.csv\n",
      "Total time: 275.169 seconds\n",
      "Preparing trace data for run ./frontier-16-node-single-HPL-run, node node0...\n",
      "Metrics will be loaded from: ./frontier-16-node-single-HPL-run/MPI Rank 0_metrics.csv\n",
      "Preparing trace data for run ./frontier-16-node-single-HPL-run, node node1...\n",
      "Metrics will be loaded from: ./frontier-16-node-single-HPL-run/MPI Rank 8_metrics.csv\n",
      "Preparing trace data for run ./frontier-16-node-single-HPL-run, node node2...\n",
      "Metrics will be loaded from: ./frontier-16-node-single-HPL-run/MPI Rank 16_metrics.csv\n",
      "Preparing trace data for run ./frontier-16-node-single-HPL-run, node node3...\n",
      "Metrics will be loaded from: ./frontier-16-node-single-HPL-run/MPI Rank 24_metrics.csv\n",
      "Preparing trace data for run ./frontier-16-node-single-HPL-run, node node4...\n",
      "Metrics will be loaded from: ./frontier-16-node-single-HPL-run/MPI Rank 32_metrics.csv\n",
      "Preparing trace data for run ./frontier-16-node-single-HPL-run, node node5...\n",
      "Metrics will be loaded from: ./frontier-16-node-single-HPL-run/MPI Rank 40_metrics.csv\n",
      "Preparing trace data for run ./frontier-16-node-single-HPL-run, node node6...\n",
      "Metrics will be loaded from: ./frontier-16-node-single-HPL-run/MPI Rank 48_metrics.csv\n",
      "Preparing trace data for run ./frontier-16-node-single-HPL-run, node node7...\n",
      "Metrics will be loaded from: ./frontier-16-node-single-HPL-run/MPI Rank 56_metrics.csv\n",
      "Preparing trace data for run ./frontier-16-node-single-HPL-run, node node8...\n",
      "Metrics will be loaded from: ./frontier-16-node-single-HPL-run/MPI Rank 64_metrics.csv\n",
      "Preparing trace data for run ./frontier-16-node-single-HPL-run, node node9...\n",
      "Metrics will be loaded from: ./frontier-16-node-single-HPL-run/MPI Rank 72_metrics.csv\n",
      "Preparing trace data for run ./frontier-16-node-single-HPL-run, node node10...\n",
      "Metrics will be loaded from: ./frontier-16-node-single-HPL-run/MPI Rank 80_metrics.csv\n",
      "Preparing trace data for run ./frontier-16-node-single-HPL-run, node node11...\n",
      "Metrics will be loaded from: ./frontier-16-node-single-HPL-run/MPI Rank 88_metrics.csv\n",
      "Preparing trace data for run ./frontier-16-node-single-HPL-run, node node12...\n",
      "Metrics will be loaded from: ./frontier-16-node-single-HPL-run/MPI Rank 96_metrics.csv\n",
      "Preparing trace data for run ./frontier-16-node-single-HPL-run, node node13...\n",
      "Metrics will be loaded from: ./frontier-16-node-single-HPL-run/MPI Rank 104_metrics.csv\n",
      "Preparing trace data for run ./frontier-16-node-single-HPL-run, node node14...\n",
      "Metrics will be loaded from: ./frontier-16-node-single-HPL-run/MPI Rank 112_metrics.csv\n",
      "Preparing trace data for run ./frontier-16-node-single-HPL-run, node node15...\n",
      "Metrics will be loaded from: ./frontier-16-node-single-HPL-run/MPI Rank 120_metrics.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                     \r"
     ]
    }
   ],
   "source": [
    "HPL_NODE_RANKS = {\n",
    "    'node0': ['MPI Rank 0', 'MPI Rank 1', 'MPI Rank 2', 'MPI Rank 3', 'MPI Rank 4', 'MPI Rank 5', 'MPI Rank 6', 'MPI Rank 7'],\n",
    "    'node1': ['MPI Rank 8', 'MPI Rank 9', 'MPI Rank 10', 'MPI Rank 11', 'MPI Rank 12', 'MPI Rank 13', 'MPI Rank 14', 'MPI Rank 15'],\n",
    "    'node2': ['MPI Rank 16', 'MPI Rank 17', 'MPI Rank 18', 'MPI Rank 19', 'MPI Rank 20', 'MPI Rank 21', 'MPI Rank 22', 'MPI Rank 23'],\n",
    "    'node3': ['MPI Rank 24', 'MPI Rank 25', 'MPI Rank 26', 'MPI Rank 27', 'MPI Rank 28', 'MPI Rank 29', 'MPI Rank 30', 'MPI Rank 31'],\n",
    "    'node4': ['MPI Rank 32', 'MPI Rank 33', 'MPI Rank 34', 'MPI Rank 35', 'MPI Rank 36', 'MPI Rank 37', 'MPI Rank 38', 'MPI Rank 39'],\n",
    "    'node5': ['MPI Rank 40', 'MPI Rank 41', 'MPI Rank 42', 'MPI Rank 43', 'MPI Rank 44', 'MPI Rank 45', 'MPI Rank 46', 'MPI Rank 47'],\n",
    "    'node6': ['MPI Rank 48', 'MPI Rank 49', 'MPI Rank 50', 'MPI Rank 51', 'MPI Rank 52', 'MPI Rank 53', 'MPI Rank 54', 'MPI Rank 55'],\n",
    "    'node7': ['MPI Rank 56', 'MPI Rank 57', 'MPI Rank 58', 'MPI Rank 59', 'MPI Rank 60', 'MPI Rank 61', 'MPI Rank 62', 'MPI Rank 63'],\n",
    "    'node8': ['MPI Rank 64', 'MPI Rank 65', 'MPI Rank 66', 'MPI Rank 67', 'MPI Rank 68', 'MPI Rank 69', 'MPI Rank 70', 'MPI Rank 71'],\n",
    "    'node9': ['MPI Rank 72', 'MPI Rank 73', 'MPI Rank 74', 'MPI Rank 75', 'MPI Rank 76', 'MPI Rank 77', 'MPI Rank 78', 'MPI Rank 79'],\n",
    "    'node10': ['MPI Rank 80', 'MPI Rank 81', 'MPI Rank 82', 'MPI Rank 83', 'MPI Rank 84', 'MPI Rank 85', 'MPI Rank 86', 'MPI Rank 87'],\n",
    "    'node11': ['MPI Rank 88', 'MPI Rank 89', 'MPI Rank 90', 'MPI Rank 91', 'MPI Rank 92', 'MPI Rank 93', 'MPI Rank 94', 'MPI Rank 95'],\n",
    "    'node12': ['MPI Rank 96', 'MPI Rank 97', 'MPI Rank 98', 'MPI Rank 99', 'MPI Rank 100', 'MPI Rank 101', 'MPI Rank 102', 'MPI Rank 103'],\n",
    "    'node13': ['MPI Rank 104', 'MPI Rank 105', 'MPI Rank 106', 'MPI Rank 107', 'MPI Rank 108', 'MPI Rank 109', 'MPI Rank 110', 'MPI Rank 111'],\n",
    "    'node14': ['MPI Rank 112', 'MPI Rank 113', 'MPI Rank 114', 'MPI Rank 115', 'MPI Rank 116', 'MPI Rank 117', 'MPI Rank 118', 'MPI Rank 119'],\n",
    "    'node15': ['MPI Rank 120', 'MPI Rank 121', 'MPI Rank 122', 'MPI Rank 123', 'MPI Rank 124', 'MPI Rank 125', 'MPI Rank 126', 'MPI Rank 127'],\n",
    "}\n",
    "\n",
    "start_convert = time.time()\n",
    "os.system(\"LD_LIBRARY_PATH=\\\"/opt/rocm-6.4.1/lib:/lustre/orion/csc688/world-shared/scorep-amd/install/lib:$LD_LIBRARY_PATH\\\" CHPL_RT_NUM_THREADS_PER_LOCALE=64 ./fast-OTF2/trace_to_csv_parallel_real -nl1 --trace ./frontier-16-node-single-HPL-run/traces.otf2 --outputDir ./frontier-16-node-single-HPL-run\")\n",
    "start_load = time.time()\n",
    "ens = Ensemble.from_trace_paths(['./frontier-16-node-single-HPL-run'], HPL_NODE_RANKS)\n",
    "end = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a1a7112e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trace conversion finished in 292.81 seconds.\n",
      "Loaded Ensemble in 23.30 seconds.\n",
      "Converted and loaded ensemble in 316.11 seconds.\n"
     ]
    }
   ],
   "source": [
    "print(f\"Trace conversion finished in {start_load - start_convert:.2f} seconds.\")\n",
    "print(f\"Loaded Ensemble in {end - start_load:.2f} seconds.\")\n",
    "print(f\"Converted and loaded ensemble in {end - start_convert:.2f} seconds.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "13df2bcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Option enabled: Skipping consecutive duplicate metric values.\n",
      "Processing events...\n",
      "CSV conversion completed in 9.000000 seconds.\n",
      "Preparing trace data for run ./frontier-1-node-single-HPL-run, node node0...\n",
      "Metrics will be loaded from: ./frontier-1-node-single-HPL-run/MPI Rank 0_metrics.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                 \r"
     ]
    }
   ],
   "source": [
    "HPL_NODE_RANKS = {\n",
    "    'node0': ['MPI Rank 0', 'MPI Rank 1', 'MPI Rank 2', 'MPI Rank 3', 'MPI Rank 4', 'MPI Rank 5', 'MPI Rank 6', 'MPI Rank 7'],\n",
    "}\n",
    "start_convert = time.time()\n",
    "os.system(\"cd frontier-1-node-single-HPL-run/ && LD_LIBRARY_PATH=\\\"/opt/rocm-6.4.1/lib:/lustre/orion/csc688/world-shared/scorep-amd/install/lib:$LD_LIBRARY_PATH\\\" ../otf2csv ./traces.otf2\")\n",
    "start_load = time.time()\n",
    "ens = Ensemble.from_trace_paths(['./frontier-1-node-single-HPL-run'], HPL_NODE_RANKS)\n",
    "end = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bec72d44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trace conversion finished in 8.80 seconds.\n",
      "Loaded Ensemble in 0.64 seconds.\n",
      "Converted and loaded ensemble in 9.44 seconds.\n"
     ]
    }
   ],
   "source": [
    "print(f\"Trace conversion finished in {start_load - start_convert:.2f} seconds.\")\n",
    "print(f\"Loaded Ensemble in {end - start_load:.2f} seconds.\")\n",
    "print(f\"Converted and loaded ensemble in {end - start_convert:.2f} seconds.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b0ee382b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Option enabled: Skipping consecutive duplicate metric values.\n",
      "Processing events...\n",
      "CSV conversion completed in 658.000000 seconds.\n",
      "Preparing trace data for run ./frontier-16-node-single-HPL-run, node node0...\n",
      "Metrics will be loaded from: ./frontier-16-node-single-HPL-run/MPI Rank 0_metrics.csv\n",
      "Preparing trace data for run ./frontier-16-node-single-HPL-run, node node1...\n",
      "Metrics will be loaded from: ./frontier-16-node-single-HPL-run/MPI Rank 8_metrics.csv\n",
      "Preparing trace data for run ./frontier-16-node-single-HPL-run, node node2...\n",
      "Metrics will be loaded from: ./frontier-16-node-single-HPL-run/MPI Rank 16_metrics.csv\n",
      "Preparing trace data for run ./frontier-16-node-single-HPL-run, node node3...\n",
      "Metrics will be loaded from: ./frontier-16-node-single-HPL-run/MPI Rank 24_metrics.csv\n",
      "Preparing trace data for run ./frontier-16-node-single-HPL-run, node node4...\n",
      "Metrics will be loaded from: ./frontier-16-node-single-HPL-run/MPI Rank 32_metrics.csv\n",
      "Preparing trace data for run ./frontier-16-node-single-HPL-run, node node5...\n",
      "Metrics will be loaded from: ./frontier-16-node-single-HPL-run/MPI Rank 40_metrics.csv\n",
      "Preparing trace data for run ./frontier-16-node-single-HPL-run, node node6...\n",
      "Metrics will be loaded from: ./frontier-16-node-single-HPL-run/MPI Rank 48_metrics.csv\n",
      "Preparing trace data for run ./frontier-16-node-single-HPL-run, node node7...\n",
      "Metrics will be loaded from: ./frontier-16-node-single-HPL-run/MPI Rank 56_metrics.csv\n",
      "Preparing trace data for run ./frontier-16-node-single-HPL-run, node node8...\n",
      "Metrics will be loaded from: ./frontier-16-node-single-HPL-run/MPI Rank 64_metrics.csv\n",
      "Preparing trace data for run ./frontier-16-node-single-HPL-run, node node9...\n",
      "Metrics will be loaded from: ./frontier-16-node-single-HPL-run/MPI Rank 72_metrics.csv\n",
      "Preparing trace data for run ./frontier-16-node-single-HPL-run, node node10...\n",
      "Metrics will be loaded from: ./frontier-16-node-single-HPL-run/MPI Rank 80_metrics.csv\n",
      "Preparing trace data for run ./frontier-16-node-single-HPL-run, node node11...\n",
      "Metrics will be loaded from: ./frontier-16-node-single-HPL-run/MPI Rank 88_metrics.csv\n",
      "Preparing trace data for run ./frontier-16-node-single-HPL-run, node node12...\n",
      "Metrics will be loaded from: ./frontier-16-node-single-HPL-run/MPI Rank 96_metrics.csv\n",
      "Preparing trace data for run ./frontier-16-node-single-HPL-run, node node13...\n",
      "Metrics will be loaded from: ./frontier-16-node-single-HPL-run/MPI Rank 104_metrics.csv\n",
      "Preparing trace data for run ./frontier-16-node-single-HPL-run, node node14...\n",
      "Metrics will be loaded from: ./frontier-16-node-single-HPL-run/MPI Rank 112_metrics.csv\n",
      "Preparing trace data for run ./frontier-16-node-single-HPL-run, node node15...\n",
      "Metrics will be loaded from: ./frontier-16-node-single-HPL-run/MPI Rank 120_metrics.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                     \r"
     ]
    }
   ],
   "source": [
    "HPL_NODE_RANKS = {\n",
    "    'node0': ['MPI Rank 0', 'MPI Rank 1', 'MPI Rank 2', 'MPI Rank 3', 'MPI Rank 4', 'MPI Rank 5', 'MPI Rank 6', 'MPI Rank 7'],\n",
    "    'node1': ['MPI Rank 8', 'MPI Rank 9', 'MPI Rank 10', 'MPI Rank 11', 'MPI Rank 12', 'MPI Rank 13', 'MPI Rank 14', 'MPI Rank 15'],\n",
    "    'node2': ['MPI Rank 16', 'MPI Rank 17', 'MPI Rank 18', 'MPI Rank 19', 'MPI Rank 20', 'MPI Rank 21', 'MPI Rank 22', 'MPI Rank 23'],\n",
    "    'node3': ['MPI Rank 24', 'MPI Rank 25', 'MPI Rank 26', 'MPI Rank 27', 'MPI Rank 28', 'MPI Rank 29', 'MPI Rank 30', 'MPI Rank 31'],\n",
    "    'node4': ['MPI Rank 32', 'MPI Rank 33', 'MPI Rank 34', 'MPI Rank 35', 'MPI Rank 36', 'MPI Rank 37', 'MPI Rank 38', 'MPI Rank 39'],\n",
    "    'node5': ['MPI Rank 40', 'MPI Rank 41', 'MPI Rank 42', 'MPI Rank 43', 'MPI Rank 44', 'MPI Rank 45', 'MPI Rank 46', 'MPI Rank 47'],\n",
    "    'node6': ['MPI Rank 48', 'MPI Rank 49', 'MPI Rank 50', 'MPI Rank 51', 'MPI Rank 52', 'MPI Rank 53', 'MPI Rank 54', 'MPI Rank 55'],\n",
    "    'node7': ['MPI Rank 56', 'MPI Rank 57', 'MPI Rank 58', 'MPI Rank 59', 'MPI Rank 60', 'MPI Rank 61', 'MPI Rank 62', 'MPI Rank 63'],\n",
    "    'node8': ['MPI Rank 64', 'MPI Rank 65', 'MPI Rank 66', 'MPI Rank 67', 'MPI Rank 68', 'MPI Rank 69', 'MPI Rank 70', 'MPI Rank 71'],\n",
    "    'node9': ['MPI Rank 72', 'MPI Rank 73', 'MPI Rank 74', 'MPI Rank 75', 'MPI Rank 76', 'MPI Rank 77', 'MPI Rank 78', 'MPI Rank 79'],\n",
    "    'node10': ['MPI Rank 80', 'MPI Rank 81', 'MPI Rank 82', 'MPI Rank 83', 'MPI Rank 84', 'MPI Rank 85', 'MPI Rank 86', 'MPI Rank 87'],\n",
    "    'node11': ['MPI Rank 88', 'MPI Rank 89', 'MPI Rank 90', 'MPI Rank 91', 'MPI Rank 92', 'MPI Rank 93', 'MPI Rank 94', 'MPI Rank 95'],\n",
    "    'node12': ['MPI Rank 96', 'MPI Rank 97', 'MPI Rank 98', 'MPI Rank 99', 'MPI Rank 100', 'MPI Rank 101', 'MPI Rank 102', 'MPI Rank 103'],\n",
    "    'node13': ['MPI Rank 104', 'MPI Rank 105', 'MPI Rank 106', 'MPI Rank 107', 'MPI Rank 108', 'MPI Rank 109', 'MPI Rank 110', 'MPI Rank 111'],\n",
    "    'node14': ['MPI Rank 112', 'MPI Rank 113', 'MPI Rank 114', 'MPI Rank 115', 'MPI Rank 116', 'MPI Rank 117', 'MPI Rank 118', 'MPI Rank 119'],\n",
    "    'node15': ['MPI Rank 120', 'MPI Rank 121', 'MPI Rank 122', 'MPI Rank 123', 'MPI Rank 124', 'MPI Rank 125', 'MPI Rank 126', 'MPI Rank 127'],\n",
    "}\n",
    "\n",
    "start_convert = time.time()\n",
    "os.system(\"cd frontier-16-node-single-HPL-run/ && LD_LIBRARY_PATH=\\\"/opt/rocm-6.4.1/lib:/lustre/orion/csc688/world-shared/scorep-amd/install/lib:$LD_LIBRARY_PATH\\\" ../otf2csv ./traces.otf2\")\n",
    "start_load = time.time()\n",
    "ens = Ensemble.from_trace_paths(['./frontier-16-node-single-HPL-run'], HPL_NODE_RANKS)\n",
    "end = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e8d57e38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trace conversion finished in 657.94 seconds.\n",
      "Loaded Ensemble in 20.38 seconds.\n",
      "Converted and loaded ensemble in 678.33 seconds.\n"
     ]
    }
   ],
   "source": [
    "print(f\"Trace conversion finished in {start_load - start_convert:.2f} seconds.\")\n",
    "print(f\"Loaded Ensemble in {end - start_load:.2f} seconds.\")\n",
    "print(f\"Converted and loaded ensemble in {end - start_convert:.2f} seconds.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "81a19149",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Notebook end time: 1767994863.4637973\n",
      "Notebook end at 1767994863.4637973 (2026-01-09 16:41:03.463797)\n",
      "Notebook execution time: 1016.33 seconds\n"
     ]
    }
   ],
   "source": [
    "# Get the end time of the notebook\n",
    "NOTEBOOK_END_TIME = time.time()\n",
    "print(f\"Notebook end time: {NOTEBOOK_END_TIME}\")\n",
    "print(f\"Notebook end at {NOTEBOOK_END_TIME} ({datetime.datetime.fromtimestamp(NOTEBOOK_END_TIME)})\")\n",
    "\n",
    "NOTEBOOK_DURATION = NOTEBOOK_END_TIME - NOTEBOOK_START_TIME\n",
    "print(f\"Notebook execution time: {NOTEBOOK_DURATION:.2f} seconds\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
